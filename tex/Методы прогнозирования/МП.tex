\RequirePackage{ifluatex}
\let\ifluatex\relax

\documentclass[aps,%
12pt,%
final,%
oneside,
onecolumn,%
musixtex, %
superscriptaddress,%
centertags]{article} %% 
\topmargin=-40pt
\textheight=650pt
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
%всякие настройки по желанию%
\usepackage[colorlinks=true,linkcolor=black,unicode=true]{hyperref}
\usepackage{euscript}
\usepackage{supertabular}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb, amsmath}
\usepackage{textcomp}
\usepackage[noend]{algorithmic}
\usepackage[ruled]{algorithm}
\usepackage{lipsum}
\usepackage{indentfirst}
\usepackage{babel}
\usepackage{pgfplots}
\usepackage{setspace}
\linespread{1.15}
\pgfplotsset{compat=1.9}

\pgfplotsset{model/.style = {blue, samples = 100}}
\pgfplotsset{experiment/.style = {red}}

\selectlanguage{russian}

\setlength{\parindent}{2.4em}
\setlength{\parskip}{0.1em}
%\renewcommand{\baselinestretch}{2.0}

\usepackage{xcolor}
\usepackage{hyperref}
 
 % Цвета для гиперссылок
%\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
%\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
%\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\begin{document}

\begin{titlepage} 
\begin{center}
% Upper part of the page
%\textbf{\Large САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ ЭКОНОМИЧЕСКИЙ УНИВЕРСИТЕТ} \\[1.0cm]
%\textbf{\large Кафедра Прикладной Математики и Информатики}\\[3.5cm]
 
% Title
\textbf{}\\[10.0cm]
\textbf{\LARGE Методы прогнозирования}\\[0.5cm]
\textbf{\Large ПМ-1701} \\[0.1cm]

%supervisor
\begin{center} \large
{Преподаватель:} \\[0.5cm]
\textsc {Ивахненко Дарья Александровна}\\
{viktor\_chernov@mail.ru}\\
\end{center}
% \begin{flushright} \large
%\emph{Рецензент:} \\
%д.ф. - м.н., профессор \textsc{Надеемся Нам Помогут}
%\end{flushright}
%\begin{flushright} \large
%\emph{Заведующий кафедрой:} \\
%д.ф. - м.н., профессор \textsc{Не Обмани Себя}
%\end{flushright}
\vfill 

% Bottom of the page
{\large {Санкт-Петербург}} \par
{\large {2020 г., 6 семестр}}
\end{center} 
\end{titlepage}

% Table of contents
\begin{thebibliography}{3}
\bibitem{rob}
Теория игр
\end{thebibliography}
\tableofcontents
\newpage

\section{11.02.2020}
\subsection{Введение}
Будем исследовать временные ряды и варианты его прогнозирования. В конце семестра перейдем к машинному обучению.

Мы рассматривали введение в файле и семейство преобразований Бокса-Кокса:

$$ y^{(\lambda)} = 
\left\{
\begin{matrix}
\frac{y^{\lambda}-1}{\lambda}, \lambda \neq 0 \\
\ln y, \lambda = 0
\end{matrix} \right. \eqno $$
$$ \lim_{\lambda \to 0}{\frac{y^{\lambda}-1}{\lambda}} = \lim_{\lambda \to 0}{\frac{\exp (\lambda \ln y)-1}{\lambda}} = \lim_{\lambda \to 0}{\frac{1+\lambda \ln y+O((\lambda \ln y)^2) - 1}{\lambda}} = \ln y $$

$$y^{\lambda} = \frac{y^\lambda -1}{\lambda} $$


$$
\frac{\partial y^{\lambda} }{\partial \lambda} = \frac{\lambda y^{\lambda - 1}}{\lambda} = y^{\lambda-1} $$

Параметр $\lambda$ можно найти с помощью метода максимального правдоподобия:
$$ L = \prod_{i=1}^{T} {\frac{1}{\sqrt{2\pi}\cdot \sigma}} \cdot \exp \left ( {-\frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)})^2}}{\sigma^2}}\right) \cdot J(\lambda,y) \to \max $$

Прологорифмируем:
$$ \ln L = \sum_{i=1}^{T} \left ( \ln \frac{1}{\sqrt{2\pi}\cdot \sigma}  - \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)})^2} }{2\sigma^2} + \ln J(\lambda,y) \right )  $$

Воспользуемся оценкой выборочной дисперсии:
$$ \hat{\sigma}^2 = \sum_{i=1}^{T} \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)}})^2}{T} $$
$$ \ln \frac{1}{\sqrt{2\pi \sigma^2}} = \ln \frac{1}{\sqrt{2\pi}} + \ln \frac{1}{\sqrt {\sigma^2}} = \ln \frac{1}{\sqrt{2\pi}} - \ln \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)}})^2}{T}$$ 

Тогда:
$$ \ln L = \sum_{i=1}^{T} \ln \frac{1}{\sqrt{2\pi}} - \ln \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)}})^2}{T} - \frac{T}{2} + \log \prod_{i=1}^{T} y_{i}^{\lambda - 1} = $$ 
$$ = \sum_{i=1}^{T} \ln \frac{1}{\sqrt{2\pi}} - \sum_{i=1}^{T} \ln \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)}})^2}{T} - \frac{T}{2} + (\lambda - 1) \sum_{i=1} \ln y_{i} = -\frac{T}{2} \cdot \ln \sum_{i=1}^{T} \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)}})^2}{T} + (\lambda - 1) \sum_{i=1} \ln y_{i} $$

Итого: логафрим функции правдоподобия Бокса-Кокса:

$$ \ln L = -\frac{T}{2} \cdot \ln \sum_{i=1}^{T} \frac{({y_i^{(\lambda)} - \overline{y}^{(\lambda)}})^2}{T} + (\lambda - 1) \sum_{i=1} \ln y_{i} $$

где $$\overline{y}^{(\lambda)} = \frac{1}{T} \sum y_i^{(\lambda)} $$

$ T $ - количество элементов в выборке.


Взяв производную по параметру $\lambda$ и прировняв к нулю, найдем решение, получим оценку методом максимального правдоподобия.
\section{Наивные методы прогнозирования}
\subsection{Адаптивные методы прогнозирования}
$$ y_2 = y_1$$
$$y_3 = \alpha y_2+(1-\alpha)y_1 = l_3$$
$$y_4 = \alpha y_3 + (1-\alpha)(\alpha y_2 + (1-\alpha)y_1) $$

где $1-\alpha=l_3$

С помощью $l_0$ можно использовать начальное значение. 

\section{Функционалы качества}

Пусть случайная величина $y$ распределена по следующему закону:
$$y_i = \sum_{j=0}^m \theta_j x_{ij}+ N(0,\sigma^2) = N(\sum_{j=0}^m \theta_j x_{ij},\sigma^2)$$

То есть $$\mu  = \sum_{j=0}^m \theta_j x_{ij} = \left .\bar{\theta}\right .^T \cdot \bar{x}_i $$

Найдем оценку параметра $\theta$ методом максимального правдоподобия.

Функция правдоподобия записывается следующим образом:
$$L(\mu,\sigma) = \prod_{i=1}^n p(y_i,\mu,\sigma^2) = \frac{1}{(\sqrt{2\pi}\cdot \sigma)^n} \cdot e^{-\frac{1}{2}\sum\limits_{i=1}^n \frac{(y_i - \mu)^2}{\sigma^2}} \to \underset{\theta}{\max}$$

Прологарифмируем:
$$\ln L(\mu,\sigma) = \ln \left(\frac{1}{(\sqrt{2\pi}\cdot \sigma)^n}\right) + \left(-\frac{1}{2}\sum\limits_{i=1}^n \frac{(y_i - \mu)^2}{\sigma^2}\right)  =  \ln \left((\sqrt{2\pi}\cdot \sigma)^{-n}\right) -$$
$$ -\frac{1}{2}\sum\limits_{i=1}^n \frac{(y_i - \mu)^2}{\sigma^2}  = -n \ln \sqrt{2\pi\sigma^2} - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n (y_i - \mu)^2  = $$
$$ = -\frac{n}{2} \ln (2\pi\sigma^2) -\frac{1}{2\sigma^2}\sum\limits_{i=1}^n (y_i -  \left .\bar{\theta}\right .^T \cdot \bar{x}_i)^2  \to \underset{\theta}{\max} $$
$$Q = - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n (y_i -  \left .\bar{\theta}\right .^T \cdot \bar{x}_i)^2  \to \underset{\theta}{\max}$$

Пусть величина $y \in \{0,1,2,\ldots, \}$, тогда для нее применимо распределение Пуассона:
$$y \sim Poiss(\lambda)$$

Так как $\lambda $ должно быть больше нуля, то хотелось бы линейную комбинацию $(-\infty, \infty)$ переделать в промежуток $(0,\infty)$. Это можно сделать с помощью потенцирования.
$$\log y =  \left .\bar{\theta}\right .^T \cdot \bar{x} \Rightarrow \lambda = e^{\left .\bar{\theta}\right .^T \cdot \bar{x}}$$

Опять же найдем с помощью ммп логарифм функции правдоподобия
$$P(\xi = y)  = \frac{e^{-\lambda}\cdot \lambda^y}{y!}, y \in \{0,1,2,\ldots, \} $$
$$L(\lambda) = \prod\limits_{i=1}^n p(y_i,\lambda) = \prod\limits_{i=1}^n \frac{e^{-\lambda} \lambda^{y_i}}{y_i !}$$
$$\ln L(\lambda) = \sum\limits_{i=1}^n \ln \frac{e^{-\lambda} \lambda^{y_i}}{y_i !} =\sum\limits_{i=1}^n (-\lambda + y_i \ln \lambda - \ln(y_i !))$$

Последнее слагаемое не сожержит оцениваемого параметра $\lambda$, поэтому:
$$Q =\sum\limits_{i=1}^n (-\lambda + y_i \ln \lambda ) \to \underset{\theta}{\max}$$
$$Q = \sum\limits_{i=1}^n ( e^{\left .\bar{\theta}\right .^T \cdot \bar{x}}  - y_i \ln  e^{\left .\bar{\theta}\right .^T \cdot \bar{x}} ) \to \underset{\theta}{\min}$$

В обобщенных линейных моделях ожидаемые значения отклика представляют
собой линейную комбинацию предикторов, которые связаны с зависимой переменной
через функцию связи g:
$$\mu = g^{-1}(\left .\bar{\theta}\right .^T \cdot \bar{x}_i)$$
$$g(\mu) = \left .\bar{\theta}\right .^T \cdot \bar{x}_i$$

Для нормального распределения, изначально было положено:
$$g(\mu) = \left .\bar{\theta}\right .^T \cdot \bar{x}_i = \mu$$

Для пуассоновского распределения:
$$g(\mu) = \log \lambda$$
$$\mu = g^{-1}(\log \mu) = e^{\log \mu} = \mu$$

Рассмотрим экспоненциальное семейство распределений:
$$f(y,\alpha,\varphi) = exp \left[\frac{y\cdot \alpha - c(\alpha)}{\varphi} + h(y,\phi) \right]$$

Для нормального распределения:
$$f(y,\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y-\mu)^2}{2\sigma^2}} = exp \left[-\frac{y^2}{2\sigma^2}+\frac{y \mu}{\sigma^2} -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log (2\pi \sigma^2)\right] = $$
$$ = exp \left[ \frac{\mu y - \frac{\mu^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \log (2\pi \sigma^2)\right]$$

Это эксопненциальное распределение с:
$$\alpha = \mu,  c(\alpha) = \frac{\alpha^2}{2}$$
$$E(y) = \mu  = c' (\alpha)$$
$$D(y) = \varphi \cdot c''(\alpha) = \sigma^2 \cdot 1 = \sigma^2 $$
$$g(\mu)  =[c']^{-1}(\mu)$$
$$c(\mu)' = \mu =\left .\bar{\theta}\right .^T \cdot \bar{x} $$

Для Пуассона:
$$f(y,\lambda) = exp\left(\frac{y\cdot\log\lambda - \lambda}{1} - log(y!)\right)$$
$$\alpha = \log(\lambda), \lambda = exp(\alpha), c(\alpha) = \lambda, c(\lambda) = e^{\lambda}$$
$$\log(\lambda) = \left .\bar{\theta}\right .^T \cdot \bar{x}$$
\end{document}
