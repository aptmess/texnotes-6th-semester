\RequirePackage{ifluatex}
\let\ifluatex\relax

\documentclass[aps,%
12pt,%
final,%
oneside,
onecolumn,%
musixtex, %
superscriptaddress,%
centertags]{article} %% 
\topmargin=-40pt
\textheight=650pt
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
%всякие настройки по желанию%
\usepackage[colorlinks=true,linkcolor=black,unicode=true]{hyperref}
\usepackage{euscript}
\usepackage{supertabular}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb, amsmath}
\usepackage{textcomp}
\usepackage[noend]{algorithmic}
\usepackage[ruled]{algorithm}
\usepackage{lipsum}
\usepackage{indentfirst}
\usepackage{babel}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}

\pgfplotsset{model/.style = {blue, samples = 100}}
\pgfplotsset{experiment/.style = {red}}
\selectlanguage{russian}

\setlength{\parindent}{2.4em}
\setlength{\parskip}{0.1em}
%\renewcommand{\baselinestretch}{2.0}

\usepackage{xcolor}
\usepackage{hyperref}
 
 % Цвета для гиперссылок
%\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
%\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
%\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\begin{document}

\begin{titlepage} 
\begin{center}
% Upper part of the page
%\textbf{\Large САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ ЭКОНОМИЧЕСКИЙ УНИВЕРСИТЕТ} \\[1.0cm]
%\textbf{\large Кафедра Прикладной Математики и Информатики}\\[3.5cm]
 
% Title
\textbf{}\\[10.0cm]
\textbf{\LARGE Эконометрика}\\[0.5cm]
\textbf{\Large ПМ-1701} \\[0.1cm]

%supervisor
\begin{center} \large
{Преподаватель:} \\[0.5cm]
\textsc {Курышева Светлана Владимировна }\\
%{viktor\_chernov@mail.ru}\\
\end{center}
% \begin{flushright} \large
%\emph{Рецензент:} \\
%д.ф. - м.н., профессор \textsc{Надеемся Нам Помогут}
%\end{flushright}
%\begin{flushright} \large
%\emph{Заведующий кафедрой:} \\
%д.ф. - м.н., профессор \textsc{Не Обмани Себя}
%\end{flushright}
\vfill 

% Bottom of the page
{\large {Санкт-Петербург}} \par
{\large {2020 г., 6 семестр}}
\end{center} 
\end{titlepage}

% Table of contents
\begin{thebibliography}{3}
\bibitem{eliseeva}
Эконометрика: Учебник/И.И.Елисеева и др.-М.:Проспект, 2009
\bibitem{praktikuma}
Практикум по эконометрике: Учебное пособие/И.И.Елисеева и др.,М.:Финансы и статистика,2006 
\bibitem{dop1}
Эконометрика: Учебник/В. С.Мхитарян и др.-М.:2008
\bibitem{dop2}
Доугерти К. Введение в эконометрику: Учебник. 2-е изд. / Пер. с англ. – М.: ИНФРА – М, 2007
\bibitem{dop3}
Берндт Э. Практика эконометрики: классика и современность. М.,2005
\end{thebibliography}
\tableofcontents
\newpage
\section{07.02.2020}
\subsection{Общие понятия об эконометрике} 
\textbf{Эконометрика} - это наука, которая дает конкректное количественное выражение закономерностям и взаимосвязям экономических явлений и процессов с помощью статистико-математических методов и моделей.

\underline{Связь эконометрики с другими науками:}
\begin{itemize} 
  \item Экономическая теория (сущность связи явлений)
  \item Статистика (информационная база)
  \item Математические и статистические методы:
  \begin{itemize} 
  	\item $C=k\cdot Y+L$, $0<|k|<1$ - регрессия
  	\item $r=sC+Dx+T$, где $t$ - сбережения, а $x$ - инвестиции
  	\item Если s=D, то $t=s(C+x)+T$  - уравнение двухфакторной регрессии
  	\item $Y+r=C+x$ - балансовое тождество 
  \end{itemize} 
\end{itemize}

\underline{Этапы построения эконометрической модели:}
\begin{enumerate}
	
	\item Теоретоическое описание рассмариваемого процесса
	\item Сбор данных, анализ их качества
	\item Спецификация модели
	\begin{enumerate}
	\item Выявление объясняемых ($Y$) и объясняющих ($X$) переменных 
	\item Выбор функций
	\end{enumerate}
	\item Оценка параметров модели
	\item Верификация модели (т.е проверка достоверности)
	\item Интерпертация результатов 

\end{enumerate}

\subsection{Парная регрессия и корреляция в эконометрических исследованиях}
\underline{Последовательность анализа регрессии:}
\begin{enumerate}
	\item Выбор типа математической функции при построении уравнения регрессии
	\item Оценка параметров уравнения
	\item Показатели силы связи
	\item Статистическая оценка достоверности ($F$-критерий Фишера)
	\item Интвервальная оценка параметров уравнений парной регрессии
	\item Использование модели
\end{enumerate}

\underline{Выбор функции для модели может проводиться $3$-мя способами}
\begin{enumerate}
	\item Аналитический
	\item Графический
	\item Экспериментальный
\end{enumerate}

\underline{Основные виды функций в модели парной регресии:} \\ 
$y=ax+b, y=a+\frac{b}{x}, y=a+bx+cx^2,y=ax^b, a=b^x, y=ae^{bx} $
\subsection{Предпосылки регрессионной модели}
1. Модель линейна по параметрам

2. $\mathbb{E}\xi_i = 0 \forall i$, т.е ожидание значения случайного члена должно быть равно нулю в каждом наблюдении из-за того, что каждое наблюдение не должно включать в себя смещения ни в каком из направлений.

3. $\mathbb{D}\xi_i = Const $, т.е его значение в каждом наблюдении получено из распределения с постоянной теоретической дисперсией. Также не должно быть причин , делающих его больше подверженным ошибке в одних наблюдениях по сравнению с другим. Заметим, что $$\mathbb{E}\xi_i^2 = \mathbb{D}\xi_i = \mathbb{D}\sigma_{\xi_i}^2 | \forall i $$

4. Значения случайного члена имеют взаимно независимые распределения. Случайный член не подвержен автокорреляции, т.е отсутствует систематическая связь между его значениями в любых двух наблюдениях.
Ковариация равна нулю:
$$ \sigma_{\xi_{i}\xi_{j}} = \mathbb{E}(\xi_i\xi_j) = \mathbb{E}\xi_i \cdot \mathbb{E}\xi_j = 0 | \forall i \neq j $$

5. $\xi_i \sim \mathbb{N}(0,\sigma^2)$: если случайный член нормально распределен, то распределены нормально и коэффициенты регрессии.

\subsection{Оценка параметров модели}
Рассмотрим случаи, для которых мы хотим предпооложить, что одна \textit{зависимая} переменная $Y$ определяется другими переменными, называемые \textit{объясняющими} переменными (регрессорами). Математическая зависимость, связывающая эти переменные, называется \textit{моделью регрессии}. Мы допускаем, что модель регрессии имеет факт неточности - \textit{случайный} (остаточный) член.

Начнем с рассмотрения простешей модели:
$$ Y_i = \alpha + \beta X_i + \xi_i \eqno (1) $$

$Y_i$ - значение зависимой переменной, $\alpha$ и $\beta$ - постоянные величины - параметры уравнения, $\xi_i$ - случайный член.

Задача регрессионого анализа состоит в получении оценок $\alpha$ и $\beta$ и, следовательно, в определении положения прямой по точкам $\Leftrightarrow$ нужно посроить прямую, в наибольшей степени соответствующую этим точкам.

$a$ - отсечение $Y$ - оценка $\alpha$

$b$ - угловой коэффициент  - оценка $\beta$

Пусть $$\widehat{Y} = a+bX_i \eqno (2)$$ 
оцениваемая модель, а $Y_i$ - оцененное значение $Y$. Наша задача заключается в том, чтобы выяснить, существут ли способы оценки коэффициентов $a,b$ алгебраическим путем.

Обозначим за $$e_i = Y_i - \widehat{Y_i} =Y_i - a - bX_i \eqno (3)$$

Остаток наблюдений зависит от выбора коэффициентов $a$ и $b$ $\Rightarrow$ задача заключается в том, чтобы выбрать такие $a$ и $b$, предсказанное значение функции от искомой в каждой точке было минимальным. Глупо минимизировать сумма остатков, потому что при выборе выборочного среднего модели: $$\sum e_i = 0 \eqno (4)$$ 

Поэтому будем минимизировать сумму квадратов остатков. Данный метод называется \textit{Методом Наименьших Квадратов } или сокращенно МНК.
\subsubsection {Метод наименьших квадратов}

Пусть у нас имеются $n$ наблюдений $(X_i,Y_i)$, $Y$ зависит от $X$ и мы хотим подобрать уравнение: $$\widehat{Y} = a+bX_i$$

Запишем формально нашу задачу в обозначениях метода наименьших квадратов (МНК):

$$ S = \sum (Y_i-\widehat{Y_i})^2 = \sum (Y_i - a - bX_i)^2 \to min \eqno (5) $$
$$\left\{
\begin{matrix}
\frac{\partial S }{\partial a} = -2 \sum Y + 2na+2b\sum X = 0 \\[0.3cm]

\frac{\partial S }{\partial b} = -2 \sum YX + 2a\sum X+2b\sum X^2 = 0 
\end{matrix} \right. \eqno (6)$$

Применение метода наименьших квадрато приводит к системе уравнений, которая для линейных уравнений имеет вид: \\
$$ \left\{
\begin{matrix}
\sum Y = na + b\sum X \\[0.3cm]
\sum YX = a\sum X + b\sum X^2 \\
\end{matrix} \right. \eqno (7) $$

Решим данную систему линенйных уравнений методом Крамера:
$$ \delta =
\begin{vmatrix}
n & \sum X \\
\sum X & \sum X^2\\
\end{vmatrix}
= n\cdot \sum X^2 - (\sum X)^2 $$

$$ \delta_{b} =
\begin{vmatrix}
n & \sum Y \\
\sum X & \sum XY\\
\end{vmatrix} = n\cdot \sum XY - \sum X\sum Y $$
$$ b = \frac{\delta_{b}}{\delta} = \frac{n\cdot \sum XY - \sum X\sum Y}{n\cdot \sum X^2 - (\sum X)^2} = \frac{ \frac{\sum XY}{n} -\frac {\sum X\sum Y}{n}}{\frac{\sum X^2}{n} - \frac{(\sum X)^2}{n^2}} = \frac {\overline{XY} - \overline{X} \cdot \overline{Y}}{\overline{X^2} - (\overline{X})^2} = \frac {cov(x,y)}{\sigma_x^2} $$

Итого получаем коэффициенты предполагаемой модели:
$$ b = \frac {cov(x,y)}{\sigma_x^2}  = \frac{\sum (X_i - \overline{X})\cdot (Y_i - \overline{Y})}{\sum (X_i-\overline{X})^2} = \frac {\overline{XY} - \overline{X} \cdot \overline{Y}}{\overline{X^2} - (\overline{X})^2} \eqno (8) $$
$$ a = \overline{Y} - b\overline{X} \eqno (9) $$

$b$ - наклон линии регресии (коэффициент регрессии) - абсолютный показатель силы связи.

Свойства метода МНК (результаты относительно регрессий, оцениваемых по обычному МНК):

\begin{enumerate}
	\item $\sum e_i = 0$
	\item $\overline{e} = 0$
	\item $ \overline{\widehat{Y}} = \overline{Y}$
	\item $\sum X_i \cdot e_i = 0$
	\item $\sum \widehat{Y_i} \cdot e_i = 0$
\end{enumerate}

Уравнение регрессии всегда дополняется обязательным показателем тесноты связи. При использовании линейной регресии в качестве такого показателя выступает \textit{линеный коэффициент корреляции $r_{xy}$}.
Существует разные модификации формулы линейного коэффициента корреляции:
$$
r = b \frac{\sigma_x}{\sigma_y} =\frac{cov(x,y)}{\sigma_x^2} \cdot \frac{\sigma_x}{\sigma_y} =  \frac{\overline{YX} - \overline{Y} \cdot \overline{X}}{\sigma_x \sigma_y}, -1 \leq r \leq 1 \eqno (10)
$$

Шкала значений коэффициента корреляции (все значения берутся по модулю):
\begin{itemize}
	\item $r \leq0.3$ - связь слабая
	\item $0.3 < r \leq 0.5$ - связь умеренная
	\item $0.5 < r \leq 0.7$ - связь заметная
	\item $0.7 < r \leq 0.9$ - связь высокая
	\item $0.9 < r \leq 1$ - связь весьма высокая, близкая к функциональной
\end{itemize}

Следует иметь ввиду, что величина линейного коэффициента корреляции оценивает тесноту связи рассматриваемых признаков в её линейной форме. Поэтому близость абсолютной величины линейного коэффициента корреляции к нулю \textit{еще не означает отсутствие связи} между признаками.

Для оценки качества подбора линейной функции расчитывается квадрат линейного коэффициента корреляции $r_{yx}^2$, называемый \textbf{коэффициентом детерминации}. Коэффициент детерминации характеризует долю дисперсии результативного признака $y$, объясняемую регрессией, в общей дисперсии результативного признака.
$$r_{yx}^2 = \frac{\sigma_{y,obyasn}^2}{\sigma_{y,obch}^2} \eqno (11)$$

\subsubsection{Качество модели: коэффициент детерминации}
Цель регрессии - объяснение поведения $Y$. В любой выборке $Y$ оказываетяс низким, а в других - высоким. Разброс значений $Y$ можно описать с помощью суммы квадратов отклонений от выборочного среднего.
$$ \sum (Y - \overline{Y})^2 $$

Все показатели корреляции основаны на правиле сложения дисперсий $\Rightarrow$ можно разложить \textbf{общую сумму квадратов отклонений } переменной $Y$ от среднего значения $\overline{Y}$ на две части - \textbf{"объясненную" } сумму квадратов и \textbf{"необъясненную"}. 
$$\sum (Y - \overline{Y})^2 = \sum (\widehat{Y}-\overline{Y})^2 + \sum (Y - \widehat{Y})^2 \eqno (12)$$

Данное равенство можно переписать как:
$$SS_T = SS_R + SS_E \eqno (13)$$

где: 

$SS_T  = \sum (Y - \overline{Y})^2 $  - общая сумма квадратов отклонений \textit{(total sum of squares)}

$SS_R = \sum (\widehat{Y}-\overline{Y})^2 $ - \textbf{сумма квадратов отклонений, объясненная} регрессией, \textbf{факторная сумма} \textit{(sum of square due to regression)}

$SS_E = \sum (Y - \widehat{Y})^2  = \sum e_i^2 $ - \textbf{остаточная сумма} квадратов отклонений,
\textit{(sum of square due to error)}.

Введем \textbf{коэффициент детерминации}:
$$ R^2 = r^2 = \frac{\sigma_{y,obyasn}^2}{\sigma_{y,obch}^2} = \frac{SS_R}{SS_T} = 1 - \frac{SS_E}{SS_T} = 1 - \frac{\sum (Y - \widehat{Y})^2}{\sum (Y - \overline{Y})^2} $$
$$ R^2 = \frac{\sum (\widehat{Y}-\overline{Y})^2}{\sum (Y - \overline{Y})^2} = 1 - \frac{\sum (Y - \hat{Y})^2}{\sum (Y - \overline{Y})^2} \eqno (14) $$

\textbf{Коэффициент детерминации}- обобщающий показатель оценки качества построенного уравнения регрессии.
\subsubsection{Статистическая оценка достоверности регрессионной модели}

После того как найдено уравнение линейной регресии, проводится оценка значимости как уравнения в целом, так и отдельных его параметров. Оценка значимости уравнения регрессии в целом дается с помощью $F$-критерия Фишера. При этом выдвигается нулевая гипотеза, что коэффициент регрессии равен нулю, т.е $b=0$ и, следовательно, фактор $x$ не оказывает влияния на результат $Y$.

Выберем нулевую гипотезу, по которой мы будем оценивать качество модели (в генеральной совокупности):
\begin{center}
	$H_0$: $r^2 = 0 $  \\
	$H_1$: $r^2 \neq 0 $ 
\end{center}

Если же прочие факторы не влияют на результат, то $Y$ связан с $X$ функционально и остаточная сумма квадратов $SS_E = \sum e_i^2 = 0$. В этом случае сумма квадратов отклонений равна объясненной сумме квадратов: $$SS_T = SS_R$$

Поскольку не все точки поля корреляции лежат на линии регрессии, то всегда имеет место их разброс как обусловленный влиянием фактора $X$, т.е регрессией $Y$ по $X$, так и вызванный действием прочих причин(необъясненная вариация). 

Так как  $$ R^2 = 1 - \frac{\sum (Y - \widehat{Y})^2}{\sum (Y - \overline{Y})^2}  = 1 - \frac{SS_E}{SS_T},$$ 

то если $SS_T$ будет больше остаточной суммы квадратов $SS_E$, то уравнение регрессии статистически значимо и фактор $X$ оказывает существенное воздействие на результат $Y$. Это равносильно тому, что коэффициент детерминации $R^2$ будет приближаться к единице.

Любая сумма квадратов отклонений связана с числом степени свободы (\textit{df - degrees of freedom}), т.е числом свободы независимого варьирования признака. Число степеней свободы связано с числом единиц совокупности $n$ и с числом определяемых по ней констант.

При расчете объясненной или факторной суммы квадратов $\sum (\widehat{Y} - \overline{Y})^2$ используются теоретические (расчетные) значения результативного признака $\widehat{Y}$, найденные по линии регресии: $$\widehat{Y} = a+bX_i$$ 

Сумма квадратов отклонений, обусловленных линейной регрессией (следует из формулы линейного коэффициента корреляции): 
\label{SSR}
$$ SS_R = \sum (\widehat{Y_i}-\overline{Y})^2 = b^2 \cdot \sum(X-\overline{X})^2 \eqno (15)$$

так как по формулам (11) и (14):

$$ r_{yx}^2 = \frac{\sigma_{y,obyasn}^2}{\sigma_{y,obch}^2} = \frac{SS_R}{SS_T}=\frac{\sum (\widehat{Y}-\overline{Y})^2}{\sum (Y - \overline{Y})^2} = b^2 \cdot \frac{\sigma_x^2}{\sigma_y^2}  
$$
$$\sum (\widehat{Y}-\overline{Y})^2 = b^2 \cdot \frac{\sigma_x^2}{\sigma_y^2} \cdot \sum (Y - \overline{Y})^2 =b^2 \cdot \frac{\sum (X - \overline{X})^2}{\sum (Y - \overline{Y})^2} \cdot \sum (Y - \overline{Y})^2 = b^2 \cdot \sum (X - \overline{X})^2 
$$

Данная сумма квадратов отклонений имеет 1 степень свободы, так как зависит только от одной константы коэффициента регрессии $b$, следовательно: $$df_{SS_R} = 1 $$

Число степеней свободы остаточной суммы квадратов при линейной регрессии: $$df_{SS_E}=n-2$$

Число степеней свободы для общей суммы квадратов определяется числом единиц, и поскольку мы используем среднюю вычисленную по данным выборки, то теряем одну степень свободы, следовательно: $$df_{SS_T} = n-1$$

В случае линейной регрессии получаем следующее равенство: $$ n-1 = 1 + (n-2)$$

В общем случае: $$ df_{SS_T} = df_{SS_R} + df_{SS_E}$$ $$ n-1 = m + ( n - 1 - m) $$ 
$$df_{SS_T}=n-1, df_{SS_R} = m, df_{SS_E} = n-1-m \eqno (16)$$
где $m$ - число параметров переменных.

Разделив каждую сумму квадратов на соответствующее ей число степеней свободы, получим средний квадрат отклонений или \textbf{дисперсию на одну степень свободы}:

$$ MS_R = \frac {SS_R} {df_R} = \frac{\sum (\widehat{Y}-\overline{Y})^2} {m} \eqno (17)$$
$$ MS_E = \frac {SS_E} {df_E} = \frac{\sum (Y - \widehat{Y})^2} {n - 1 - m} \eqno (18)$$
$$ MS_T = \frac {SS_T} {df_T} = \frac{\sum (Y - \overline{Y})^2}{n-1}\eqno (19) $$
где $MS_T$ - общая дисперсия, $MS_E$ - остаточная, $MS_R$ - факторная (объясненная).

Определение дисперсии на одну степень свободы приводит дисперсии к \textit{сравнимому виду}. Сопоставляя факторную (объясненную) и остаточную дисперсию в расчете на одну степень свободы, получим величину \textbf{$F$-критерия}:
\label{Snedekor}
$$F=\frac {MS_R} {MS_E } =  \frac {\text{Factor Variance with 1 df} } {\text{Remainder variance with 1 df}} \eqno (20)$$

Значение $F_{table}$ означает максимальную величину отношения дисперсия при случайном их расхождении для данного уровня вероятности и наличия нулевой гипотезы.

В математической статистике данное распределение называется распределение \textit{Снедекора} для $(n,m)$ степеней свободы.

Для проверки гипотезы о значимости уравнения регресии воспользуемся следующим алгоритмом:
\begin{enumerate}
	\item Выберем в достоверной области критический уровень значимости $\alpha$. Обычно выбирают маленький уровень значимости, так как вероятность попадания в критическую область при справедливости нулевой гипотезы $H_0$ должна быть маленькой ($\alpha \approx 0.05$).
	\item Определяется табличное критическое значение критерия Фишера $F_{table}(m,n-1-m)$
	\item Если $F>F_{table}$, то $H_0$ отвергается $\Rightarrow$ гипотеза о случайности природы отвергается и делается вывод о существенности связи и значимости $R^2$
\end{enumerate}

Если нулевая гипотеза справедлива, то факторная (объясненная) и остаточная дисперсия не отличаются друг от друга. 
Величиная $F$-критерия связана с коэффициентом детерминации $r^2$. Факторную сумму квадратов отклонений можно представить как:
$$ SS_R = \sum (\widehat{Y_i}-\overline{Y})^2 = b^2 \cdot \sum(X-\overline{X})^2  = r_{yx}^2\cdot \sigma_y^2 \cdot n \eqno (21)$$

так как:
$$ SS_R = \sum (\widehat{Y_i}-\overline{Y})^2 = r_{yx}^2 \cdot SS_T = r_{yx}^2 \sum (Y - \overline{Y})^2 =$$
$$ r_{yx}^2 \cdot \frac{1}{n} \sum (Y - \overline{Y})^2 \cdot n = r_{yx}^2 \cdot \sigma_{y}^2 \cdot n  $$

А остаточную сумму квадратов как:
$$ SS_E = \sum (Y_i-\widehat{Y_i})^2 = (1-r_{yx}^2)\cdot \sigma_y^2 \cdot n \eqno (22)$$

так как:

$$ r_{xy}^2 = 1 - \frac{SS_E}{SS_T} \Rightarrow SS_E = SS_T \cdot (1-r_{xy}^2) = (1-r_{yx}^2)\cdot \sigma_y^2 \cdot n $$

Тогда значение $F$-критерия равно:
\label{FR}
$$F = \frac {MS_R} {MS_E } = \frac{\frac {SS_R} {df_R}}{\frac {SS_E} {df_E}} = \frac{r_{yx}^2}{(1-r_{yx}^2)} \cdot \frac{n-1-m}{m} \eqno (23)$$ 
где $n$ - число единиц в совокупности, $m$ - число параметров при переменных.

Результаты факторного анализа обычно представлены в таблице дисперсионного анализа
\label{first_table_analiz}
\begin{table}[H]
	\begin{center}
		\begin{tabular}[t]{|c|c|c|c|c|} \hline
		Источник вариации & df & $SS$ & $MS$ & F-критерий\\ \hline
		Регрессия & 1 & 14735 & 14735 & 278 \\ \hline
		Остаток & 5 & 265 & 53 & 1 \\ \hline
		Итого & 6 & 15000 & x & x \\ \hline
		\end{tabular}
	\caption{Таблица дисперсионного анализа для примера}
	\end{center}
\end{table}
$F_{table} = 6.61$, 278 > 6.61 - регресия статистически значима, $r^2 \neq 0$
\subsubsection{Оценка значимости коэффициентов регрессии} 

В линейной регресии обычно оценивается значимость не только уравнения в целом, но и отдельных его параметров. С этой целью по каждому из параметров строится его \textbf{стандартная ошибка} (случайная ошибка коэффициента регрессии).

Выдвигается нулевая гипотеза о равенстве коэффициентов регресии в генеральной совокупности:
$$H_0: b= 0 $$ 
$$H_1: b \neq 0 $$ 

\textbf{Стандартная ошибка} коэффициента регрессии определяется по формуле:
$$m_b= \sqrt {\frac{MS_E}{\sum (X-\overline{X})^2}} = \sqrt {\frac{\frac{\sum (Y - \widehat{Y})^2} {n - 1 - m}}{\sum (X-\overline{X})^2}} \eqno (24)$$

Вводится t-статистика:
$$ t_{b}=\frac {b - 0}{m_b} = \frac{b}{m_b} \sim t(n-2) \eqno (25)$$

так как два параметра, то число степеней свободы равно двум и данная статистика имеет распределение Стьюдента с $n-2$ степенями свободы.

Для проверки гипотезы о значимости коэффициента регресии воспользуемся следующим алгоритмом:
\begin{enumerate}
	\item Выберем в достоверной области критический уровень значимости $\alpha$. Обычно выбирают маленький уровень значимости, так как вероятность попадания в критическую область при справедливости нулевой гипотезы $H_0$ должна быть маленькой. 
	\item Определяется табличное критическое значение критерия Стьюдентая $t_{table}(n-2)$
	\item Если $|t_{b}| > t_{table}$, то $H_0$ отвергается $\rightarrow$ гипотеза о незначимости коэффициента регрессии отвергается (параметр $b$ не случайно отличается от нуля, и сформировался под влиянием систематически действующего фактора)
\end{enumerate}

Критерий опровержения гипотезы:

\begin{equation*}
	|t_{b}| = \frac {b} {m_b} = \frac {b} {\sqrt {\frac{MS_E}{\sum (X-\overline{X})^2}}} > t_{table} \Leftrightarrow H_o \text { discards} \eqno (26)
\end{equation*} 

Величина $m_b$ называется случайной ошибкой коэффициентов регресии. 
Если $t_b > 3 $, то параметры всегда значимы.
\subsubsection{Связь F и t-критериев}
$F$-критерий Снедекора и $t$-критерия Стюдента для коэффициентов регрессии взаимосвязаны. Покажем эту связь:
$$t_b^2 = \frac{b^2}{m_b^2} = \frac {b^2}{\frac{\frac{\sum (Y - \widehat{Y})^2} {n - 1 - m}}{\sum (X-\overline{X})^2}}  =  \frac {b^2 \cdot {\sum (X-\overline{X})^2}}{\frac{\sum (Y - \widehat{Y})^2} {n - 1 - m}} \stackrel{\ref{SSR}}{=} \frac{SS_R}{\frac{\sum (Y - \widehat{Y})^2} {n - 1 - m}} = \frac{MS_R}{MS_E} = F $$

Следовательно:
\label{svyaz_tb_F}
$$t_b = \sqrt {F} \eqno (27)$$

\subsubsection{Гипотеза о коэффициенте коррелляции}

Значимость линейного коэффициента корреляции проверяется на снове величины \textbf{ошибки коэффициента корреляции $m_r$}:
$$ m_r = \sqrt{\frac{1-r_{yx}^2}{n-1-m}} \eqno (28)$$

Фактическое значение $t$-критерия Стьюдента определяется как:
$$ t_r = \frac{r}{\sqrt{1-r_{yx}^2}} \cdot \sqrt{n-1-m} \eqno (29) $$
$$ F = \frac{r_{yx}^2}{(1-r_{yx}^2)} \cdot {(n-1-m)}$$

Для парной регрессии:
$$ t_r = \frac{r}{\sqrt{1-r_{yx}^2}} \cdot \sqrt{n-2} \eqno (30)$$
$$ F = \frac{r_{yx}^2}{(1-r_{yx}^2)} \cdot {(n-2)}$$

Следовательно F и t связаны для коэффициентов корреляции:
$$t_r = \frac{r_{xy}}{m_r}$$
$$ t_r^2 = F , t_b^2 = F \Rightarrow t_r^2 = t_b^2$$
$$ t_r = t_b \eqno (31)$$

Таким образом, проверка гипотез о значимости коэффициентов регрессии и корреляции равносильна проверке гипотезы о существенности линейного уравнения регрессии.
В гипотезе о корреляции: если гипотеза неверна, то зависимость является достоверной и коэффициент корреляции существенно отличен от нуля.

Рассмотренная формула оценки коэффициента корреляции работает при большом числе наблюдений и если $r$ не близко к $\pm 1$. Если же величина коэффициента корреляции близка к $1$, то распределение его оценок отличается от нормального или распределения Стьюдента, так как величина коэффициента корреляции ограничена $[-1,1]$. 

Чтобы обойти это затруднение было предложено для оценки существенности $r$ ввести вспомогательную величину $z$, связанную с коэффициентом корреляции следующим отношением:
$$ z = \frac{1}{2} \cdot ln \frac{1+r}{1-r} \eqno (32)$$ 

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[title = z-grid,  grid = none,scale=1,xmin=-1,xmax=1,ymin = -1,ymax = 1, axis lines=middle, enlargelimits,xlabel = {$x$}, ylabel = {$y$},]
		\addplot[model] {log10((1+x)/(1-x))} ;
		\end{axis}
	\end{tikzpicture}
\end{center}

Величина $z$ изменяется от $ -\infty $ до $+\infty$, что соответствуе пределам нормального распределения. 

Стандартная ошибка величины $z$ вычисляется по формуле:
$$ m_z = \frac{1}{\sqrt{n-3}} \eqno (33)$$

Далее выдвигается нулевая гипотеза $H_0$, которая состоит в том, что корреляция отсутствует, т.е теоретическое значение коэффициента корреляции равно 0:
$$ H_0: r_{xy} = 0, H_1: r_{xy} \neq 0 $$

Критерий опровержения гипотезы:
$$ t_z = \frac{z}{m_z} = z \cdot \sqrt{n-3} \sim t(n-2) \eqno (34) $$
$$ t_z > t_{\alpha} \Leftrightarrow H_0 \text{ discards} $$

\textbf{Вывод}: таким образом, если $H_0$ отвергается, то коэффициент корреляции значимо отличен от нуля.

\subsubsection{Доверительные интервалы для коэффициентов регрессии}
Если коэффициенты регрессии оказываются статистически значимыми, то можно построить \textbf{доверительный интервал} для коэффициентов регрессии:
$$\delta_b = \pm t_{table} \cdot {m_b}$$
$$ b - t_{1 - \frac{\alpha}{2}}(n-2) \cdot {m_b} \leq b \leq b + t_{1 - \frac{\alpha}{2}}(n-2) \cdot {m_b} \eqno (35)$$

Также стандартную среднюю ошибку для коэффициента $a$ можно выразить через $m_b$:
$$ m_a = \sqrt{\frac{\sum (Y - \widehat{Y})^2} {n - 1 - m} \cdot \frac{\sum X^2}{n\cdot (X-\overline{X})}} = m_b \cdot \sqrt{\frac{\sum X^2}{n}} \eqno (36)$$
\subsubsection{Использование модели парной регрессии для прогнозирования}
В прогнозных расчетах по уравнению регрессии определяется предсказываемое ($y_p$) значение как точечный прогноз $\widehat{y_x}$ при $x_p=x_k$, т.е путем подстановки в уравнение регрессии $\widehat{y_x} = a+b \cdot x$ соответствующего значения $x$. Однако точечный прогноз явно нереален, поэтому он дополняется расчетом стандартной ошибки $\widehat{y_i}$, т.е $m_{\widehat{y}}$ и соответственно интервальной оценкой прогнозированного значения $y^*$.

Выражение для \textbf{стандартной ошибки предсказываемого по линии регрессии значения} $\widehat{y}$:
$$ m_{\widehat{y_x}} = \sqrt{MS_E} \sqrt{\frac{1}{n}+\frac{(x_k-\overline{X})^2}{\sum(X-\overline{X})^2}} \eqno (37) $$

где $\sqrt{MS_E}$ - стандартная ошибка линейной регресии.
Данная формула стандартной ошибки предсказываемого значения $y$ при заданном значении $x_k$ и характеризует ошибку положения линии регрессии.

Величина стандартной ошибки достигает минимума при $x_k  = \overline{X} $.

Для прогнозируемого значения $\widehat{y}$ доверительный интервал выглядит следующим бразом:
$$ \widehat{y_{x_k}} \pm t_{1-\frac{\alpha}{2}}(n-2) \cdot m_{\widehat{y_x}} $$
$$ \widehat{y_{x_k}} - t_{1-\frac{\alpha}{2}}(n-2) \cdot m_{\widehat{y_x}} \leq \widehat{y_{x_k}} \leq \widehat{y_p} + t_{1-\frac{\alpha}{2}}(n-2) \cdot m_{\widehat{y_x}}  \eqno (38)$$

где:
$$ \widehat{y_{x_k}} = a+b\cdot x_k $$
\textbf{Средняя ошибка прогнозируемого индивидуального значения} составит:
$$ m_y = \sqrt{MS_E} \sqrt{1+\frac{1}{n}+\frac{(x_k-\overline{X})^2}{\sum(X-\overline{X})^2}} \eqno (39)$$

\textbf{Доверительный интервал для $y_p$ }- предсказываемого значения регрессии:
$$ \widehat{y_p} - t_{1-\frac{\alpha}{2}}(n-2) m_y \leq y_p \leq \widehat{y_p} + t_{1-\frac{\alpha}{2}}(n-2) m_y \eqno (40) $$

\subsubsection{Пример использования полученных знаний}

Рассмотрим выборку $\{X,Y\}$, где:
\label{formula2}
$$ X = \{1,2,4,3,5,3,4\} $$ 
$$ Y = \{30,70,150,100,170,100,150\} $$ 
Последовательно проведем анализ согласно изучению материала:

\begin{center}\textbf{1. Найдем оценку параметров модели методом МНК:}\end{center}

Согласно формуле (7) получаем следующую систему уравнений:
$$ \left\{
\begin{matrix}
770 = 7a + 22b \\
2820 = 22a + 80b \\
\end{matrix} \right. $$

Из данной системы уравнений находим значения параметров регрессии $a$ и $b$:
$$ a = -5.78947; b = 36.8421$$

Можно убедиться, что все альтернативные формулы (8) дают те же значения коэффициентов линейной регрессии. 

Построим график прямой $$\widehat{Y} = -5.78947 + 36.8421X$$
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[xmin = 0, xmax = 5, grid = major,scale = 1.5,legend pos = north west]
		\legend{ 
	Уравнение регрессии, 
	Начальные данные
	};
	\addplot[model]{-5.78947+36.8421*x};
	\addplot[scatter,only marks] coordinates { (1,30) (2,70) (4,150) (3,100) (5,170) (3,100) (4,150)} ;
		\end{axis}
	\end{tikzpicture}
\end{center}

Линейный коэффициент корреляции по формуле (10):
$$ r = 0.991189$$

\textbf{Вывод}: связь очень высокая и близкая к функциональной.

\begin{center}
\textbf{2. Качество модели} 
\end{center}

По формуле (13) найдем общую сумму квадратов отклонений и объясненную и необъясненную дисперсию:
$$ SS_T = 15000 , SS_R = 14736.8, SS_E = 263.158$$
$$ SS_T = SS_R + SS_E:  \text{ True}$$ 
По формуле (11) и (14) найдем \textit{коэффициент детерминации}:
$$ R^2 = \frac{SS_R}{SS_T} = 0.982456$$ 

\textbf{Вывод}: уравнением регрессии объясняется около $98\%$ дисперсии результативного признака, а на долю других факторов уходит лишь $2\%$ ее дисперсии. Чем больше коэффициент детерминации, тем меньше роль прочих факторов и, следовательно, линейная модель хорошо аппроксимирует данные и ею можно пользоваться для прогноза значений $Y$.

\begin{center}
\textbf{3. Проверка гипотезы о достоверности регрессионной модели} 
\end{center}

Допустим, что $H_0: r^2 = 0$ (как следствие, коэффициент $b=0$).

Выберем уровень значимости: $\alpha = 0.05$

Согласно формулам (17-19), высчитаем дисперсию на одну степень свободы :
$$ MS_R = \frac{SS_R}{1} = 14736.8 $$
$$ MS_E = \frac{SS_E}{n-1-m} = \frac{263.158}{7-2} = 52.6316 $$

Посчитаем статистику $F$-критерия: $$ F_{stat} = \frac{MS_R}{MS_E} = \frac{14736.8}{52.6316} = 280$$

Найдем табличное значение распределения Фишера-Снедекора при заданном уровне значимости:
$$ F_{1-\alpha}(m,n-1-m) = F_{0.95}(1,5) = 6.61$$

\textbf{Вывод}: так как $F_{stat}  > F_{0.95} $, то нулевая гипотеза $H_0$ отвергается и делается вывод о том, что регрессия статистически значима и связь существенна.

Также можно проверить, что значение $F$-критерия одинаково и при других альтернативе формулы (23)(можно проверить). Дисперсионная таблица представлена на странице \pageref{first_table_analiz}.

\begin{center}
\textbf{4. Проверка гипотезы о достоверности регрессионной модели} 
\end{center}

Проверим значимость отдельных параметров регрессии, в данном случае значимость параметра $b$.

Введем нулевую гипотезу о том, что параметр регрессии незначим:
\begin{center} $H_0: b=0$. \end{center}

Выберем уровень значимости: $\alpha = 0.05$

Вычислим стандартную ошибку по формуле (24), чтобы построить $t_{stat}$:
$$ m_b = \sqrt { \frac{52.6316}{10.8571}} = 2.20174 $$

Вычислим $t_{stat}$ по формуле (25) :
$$t_b = \frac{b}{m_b} = \frac{36.8421}{2.20174} = 16.7332 $$
$$t_b = \sqrt {F} = \sqrt{280} = 16.7332 $$

Вычислим табличное значение распределения Стьюдента с $(n-2)$ степенями свободы:
$$ t_{table} = t_{1 - \frac{\alpha}{2}} = t_{0.975} = 2.57058$$  

\textbf{Вывод}: так как $t_b > t_{table}$, то нулевая гипотеза $H_0$ отвергается и делается вывод, что коэффициент линейной регрессии $b$ статистически значим

Доверительный интервал для коэффицициента $b$ выглядит следующим образом, согласно формуле (35):
$$36.8421 - 2.57058 \cdot 2.20174 \leq b \leq 36.8421 + 2.57058 \cdot 2.20174  $$
$$31.1824 \leq b \leq 42.5019 $$

\begin{center}
\textbf{5. Использование модели парной регрессии для прогнозирования} 
\end{center}

Вычислим стандартную ошибку предсказываемого по линии регрессии значения $\hat{Y}$ по формуле (37):
$$ m_{\widehat{y_x}} = \sqrt{52.6316} \sqrt{\frac{1}{7} + \frac{(x_k-3.14286)^2}{10.8571}} $$

Подставляя различные значения из выборки $X$ мы можем узнать ошибку предсказываемого значения. Минимальная ошибка будет при подстановке $x_k = \overline{X}=3.14286$:
$$ m_{y_{\overline{X}}} = \sqrt{52.6316} \sqrt{\frac{1}{7}} = 2.74204$$

Построим доверительный интервал для $\widehat{Y}$ при каком-то произвольном значении $x_k$, например $x_k = 4$. Воспользуемся формулой (38).

Сначала вычислим значение линейной регресии в точке $x_k=4$:
$$ \widehat{y_{4}} = -5.78947 + 36.8421\cdot4 = 141.579$$

Затем вычислим стандартную ошибку в точке $x_k=4$:
$$ m_{\widehat{y_4}} = \sqrt{52.6316} \sqrt{\frac{1}{7} + \frac{(4-3.14286)^2}{10.8571}} = 3.32871$$

Теперь можно и построить доверительный интервал для уровня значимости $\alpha=0.05$:
$$ \widehat{y_{4}} - t_{0.975}\cdot m_{\widehat{y_4}} \leq \widehat{y_{4}} \leq \widehat{y_{4}} + t_{0.975}\cdot m_{\widehat{y_4}}$$
$$133.022 \leq \widehat{y_{4}} \leq150.136 $$

Значения будут удаляться от линии регрессии по гиперболе, с минимум ошибки в точке $x_k = \bar{X}$. Изобразим это на графике:

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[xmin = 0.8, legend pos = north west, scale = 2, xmax = 5.1,title=График стандартной ошибки и доверительные интервалы, grid = major]
		\legend{ 
	Уравнение регрессии, 
	Начальные данные, 
	Нижняя граница доверительного интервала, 
	Верхняя граница доверительного интервала
	};
	\addplot[model]{-5.78947+36.8421*x};
	\addplot[scatter,only marks] coordinates { (1,30) (2,70) (4,150) (3,100) (5,170) (3,100) (4,150)} ;
	\addplot[orange] coordinates {(0.6, 0.290471) (0.7, 4.48097) (0.8, 8.66721) (0.9, 12.8487) (1,17.0251) (2,58.328) (3,97.642) (4,133.022) (5,165.765) (5.1, 168.976) (5.2, 172.179) (5.3, 175.376) (5.4, 178.567) (5.5, 181.754) (5.6, 184.935) (5.7, 188.113) (5.8, 191.286) (5.9, 194.456) (6, 197.623) } ;
	\addplot[red] coordinates { (0.6, 32.3411) (0.7, 35.519) (0.8, 38.7012) (0.9, 41.8881) (1,45.0802) (2,77.4615) (3,111.832) (4,150.136) (5,191.077)} ;
		\end{axis}
	\end{tikzpicture}
\end{center}


Вычислим среднюю ошибку прогноза по формуле (39) и построим доверительный интвервал. Все действия аналогичны разобранному пункту.

Средняя ошибка прогноза:
$$ m_{\widehat{y_x}} = \sqrt{52.6316} \sqrt{1 + \frac{1}{7} + \frac{(x_k-3.14286)^2}{10.8571}} $$

Доверительный интервал для $x_k=4$ и уровня значимости $\alpha=0.05$ по формуле (40):

$$121.061 \leq \widehat{y_{x_{k=4}}} \leq162.097 $$

Построим график средней ошибки в зависимости от наблюдений.

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[xmin = 0.8, legend pos = north west, scale = 2, xmax = 5.1,title=График средней ошибки и доверительные интервалы, grid = major]
		\legend{ 
	Уравнение регрессии, 
	Начальные данные, 
	Нижняя граница доверительного интервала, 
	Верхняя граница доверительного интервала
	};
	\addplot[model]{-5.78947+36.8421*x};
	\addplot[scatter,only marks] coordinates { (1,30) (2,70) (4,150) (3,100) (5,170) (3,100) (4,150)} ;
	\addplot[orange] coordinates { (1,7.71691) (246.9351) (3,84.7839) (4,121.061) (5,155.883) } ;
	\addplot[red] coordinates { (1,54.3884) (2,88.8544) (3,124.69) (4,162.097) (5,200.959)} ;
		\end{axis}
	\end{tikzpicture}
\end{center}

\textbf{Вывод}: таким образом, было разобрано, как делать доверительные интвералы для ошибки прогнозирования с помощью стандартной ошибки и средней ошибки прогнозирования.
\newpage
\subsection{Нелинейная регрессия}

Если между явлениями существуют нелинейные соотношения, то они выражаются с помощью соответствующих нелинейных функций.

Различают два класса \textit{нелинейной регрессии}:
\begin{enumerate}
	\item Нелинейная по независимым переменным - регрессии, нелинейные относительно включенных в анализ объясняющих переменных, но линейные по оцениваемым параметрам.
	\item Нелинейная по оцениваемым параметрам 
\end{enumerate}

Примеры нелинейной регрессии по независимым переменным:
\begin{enumerate}
	\item полиномы разных степеней: $ y = a + bx + cx^2+ \varepsilon $
	\item равносторонняя гипербола: $ y = a + \frac{b}{x} + \varepsilon $
\end{enumerate}

Примеры нелинейных регрессий по оцениваем параметрам:
\begin{enumerate}
	\item степенная: $ y = a \cdot x^b \cdot \varepsilon $
	\item показательная: $ y = a \cdot b^x \cdot \varepsilon $
	\item экспоненциальная: $ y =e^{a+bx} \cdot \varepsilon $ 
\end{enumerate}
\subsubsection{Сведение нелинейной регрессии по независимым параметрам}

Данный класс нелинейной регресии определяется, как и в линейной регресии, МНК, ибо эти функции \textit{линейны по параметрам}. Рассмотрим, каким образом возможно перевести каждый тип к виду линейной регрессии.

\begin{center} 1. Полиномы разных степеней \end{center}

Парабола:
$$y = \alpha+ \beta x+\gamma x^2+\varepsilon$$

Замена переменных:
$$ x = x_1, x^2 = x_2 $$

Линейный вид:
$$ y = \alpha+ \beta x_1+\gamma x_2 +\varepsilon $$
\newpage

\begin{center} 2. Равносторонняя гипербола \end{center}

Модель:
$$y = \alpha+\frac{\beta}{x}+\varepsilon$$

Замена переменных:
$$ z = \frac{1}{x} $$

Линейный вид:
$$y = \alpha+ \beta \cdot z+\varepsilon$$

Примерами нелинейных регрессий являются:

1. Кривая Филлипса - отображает зависимость между уровнем безработицы $x$ и процентным изменением заработной платы $y$ 

2. Кривая Энгеля - отображает зависимость доли расходов на непродовольственные товары $y$ и общих доходов $x$

\subsubsection{Сведение нелинейной регрессии по оцениваемым параметрам}

Если модель внутренне линейна, то она с помощью соответствующих преобразований может быть приведена к линейному виду.

\begin{center} 1. Степенная функция \end{center}

Модель:
$$ y = a \cdot x^b \cdot \varepsilon $$

Логарифмируем обе части равенства (линеаризация):
$$ \ln y =\ln a + b\ln x + \ln \varepsilon $$

Замена переменных:
$$ \ln y = z, \alpha_1 = \ln a, t = \ln x, \varepsilon_1 = \ln \varepsilon $$

Линейный вид:
$$ z =\alpha_1 + b \cdot t + \varepsilon_1 $$

\begin{center} 2. Экспоненциальная модель \end{center}

Модель:
$$ y = e^{a+bx} \cdot \varepsilon $$

Логарифмируем обе части равенства:

$$ \ln y = a + bx + \ln \varepsilon$$

Замена переменных:
$$ \ln y = z, \varepsilon_1 = \ln \varepsilon $$

Линейный вид:

$$ z = a + bx + \varepsilon_1 $$

\begin{center} 3. Показательная модель \end{center}

Модель:
$$ y = a \cdot b^x \cdot \varepsilon $$

Логарифмируем обе части равенства:

$$ \ln y = \ln a + x \ln b + \ln \varepsilon$$

Замена переменных:
$$ \ln y = z, \alpha_1 = \ln a, \beta_1 = \ln b, \varepsilon_1 = \ln \varepsilon $$

Линейный вид:

$$ z = \alpha_1 + x\beta_1 + \varepsilon_1 $$

\begin{center} 4. Обратная модель \end{center}

Модель:

$$ y = \frac{1}{a+bx+\varepsilon} $$

Обращение обе части неравенства:

$$ \frac{1}{y} = a + bx + \varepsilon $$

Замена:

$$ z =\frac{1}{y} $$

Линейный вид:

$$ z = a + bx + \varepsilon $$

\subsubsection{Показатели силы связи в моделях парной регрессии}

Существует два вида показателей силы связи.

\textbf{Абсолютная} - показывает, на сколько единиц в среднем меняется результативный признак на одну единицу. В линейном уравнении параметр $b$ - абсолютный показатель силы связи.

Но существует и другой показатель силы связи. Среди нелинейных функций очень широко используется степенная функций $ y = a \cdot x^b \cdot \varepsilon $, так как параметр $b$ является коэффициентом эластичности.

\textbf{Относительные(коэффициенты эластичности)} - показывают, на сколько процентов в среднем меняется результативный признак при изменении факторного признака на $1\%$:

$$ \xi = \frac{\partial y }{\partial x} \cdot \frac{x}{y} \eqno {41}$$
% $$ \varepsilon = \frac{\mathrm{dy} }{\mathrm{d} x} \cdot{\frac{x}{y}}$$
Для степенной функции показатель эластичности равен константе, ведь:
$$\xi = \frac{\partial y }{\partial x} \cdot \frac{x}{y} = (a \cdot x^b \cdot \varepsilon)' \cdot \frac{x}{a \cdot x^b \cdot \varepsilon} = a \cdot b \cdot x^{b-1} \cdot \frac{x}{a \cdot x^b} = b \eqno {42}$$

\subsubsection{Показатели тесноты связи вмоделях нелинейной регрессии}

Коэффициент детерминации - обобщающий показатель оценки качества посроенного уравнения регресии. 

\textit{Индексом корреляции} называется следующее отношение:
$$ R =\sqrt{\frac{SS_R}{SS_T}} = \sqrt{1 - \frac{SS_E}{SS_T}} = \sqrt {1 - \frac{\sum (Y - \hat{Y_i})^2}{\sum (Y - \overline{Y})^2}} \eqno {43}$$
$$ 0 \leq R \leq 1 $$

Чем ближе индекс корреляции к $1$, тем теснее связь рассматриваемых признаков.

Если нелинейное относительно объясняемой переменной уравнение регресии при линеаризации принимает форму линейного уравнения парной регрессии, то для оценки тесноты связи может быть использован линейный коэффициент корреляции.

$$ r_{xy} = R \eqno {44}$$

\subsubsection{Средняя ошибка аппроксимации}

Величина отклонения $Y_i$ от $\widehat{Y_i}$ по каждому наблюдению представляет собой ошибку аппроксимации. Их число соответствует объему выборки. Для сравнения используются величины отклонений, выраженные в процентах к фактическим значениям.

Поскольку $Y_i - \widehat{Y_i} $ может быть величиной как положительной, так и отрицательной, то ошибки аппроксимации для каждого наблюдения принято определять в процентах по моделю.

Отклонения $Y_i - \widehat{Y_i} $ - \textit{абсолютная ошибка} аппроксимации.

$ \left| \frac{Y_i - \widehat{Y_i}}{Y_i} \right | \cdot 100\% $ - \textit{относительная ошибка} аппроксимации.

\textit{Средняя ошибка аппроксимации} (mean absolute percentage error) определяется по формуле:

$$ \text{MAPE} = \frac{\sum \left| \frac{Y_i - \widehat{Y_i}}{Y_i} \right | \cdot 100\%} {n}  \eqno {45}$$

Средняя ошибка аппроксимации должно быть примерно в интервале от $7$ до $11$ процентов.

\textit{Среднее абсолютное отклонение} (median absolute deviation) является еще одной оценкой качества уравнения регресии и вычисляется по формуле:

$$ \text{MAD} = \frac{\sum \left| Y_i - \widehat{Y_i} \right | }{n} \eqno {46}$$ 

Интервалы прогноза по нелинейной регрессии строятся по формулам, что и в линейной регрессии, заменяя переменную на исходную и приводя интервал к новому виду. В равносторонней гиперболе в расчетах используется не $x$, а $\frac{1}{x}$. В стеенной функции и экспоненте сначала определяются интервалы для $\log y$, а далее путем потенцирования находим интервалы для $y$.

Для нелинейной зависимости не выполняется равенство коэффициентов корреляции.
\section{14.02.2020 Множественная регрессия и корреляция}
\subsection{Спецификация модели}

Цель множественной регрессии состоит в том, чтобы построить модель с большим числом факторов и определить влияние каждого из них в отдельности, а также совокупное их воздействие на фактор.

Спецификация модели включает в себя два круга вопросов:

\begin{itemize}
	\item Отбор факторов - задачей является выяснить, какой фактор влияет больше всего на целевую функцию
	\item Выбор вида уравнения регрессии
\end{itemize}
\subsection{Отбор факторов}

Требования к включаемым факторам

\begin{itemize}
	\item Количественно измеримы
	\item Не должны находиться в точной функциональной связи или быть сильно коррелированы
\end{itemize}

Модель множественной регрессии в общем случае описывается данной функцией:
$$ \widehat{y} = f(x_1,x_2,...,x_k) $$

причем включаемые факторы не должны быть коррелированы, запишем это в математической формуле:
$$ r_{yx_j} > r_{x_i{x_j}} $$

Для отбора важных коэффициентов корреляции используется \textit{матрица парных коэффициентов}. 
$t_{stat}$ связано с \textit{частной корреляцией}.При анализе важности факторов, нужно провести анализ значимости коэффициентов регрессии и, если параметр незначим, то необходимо отбросить параметр с наименьшей $t_{stat}< t_{table}$.

Возникает вопрос, как оценить мультиколлинеарность факторов. Составим таблицу парных коэффициентов:
\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|} \hline
		& y & $x_1$ & $x_2$ & $x_3$ \\ \hline
		y & 1 &  &  &   \\ \hline
		$x_1$ &  & 1 & 0.95 & 0.96 \\ \hline
		$x_2$ &  & 0.95 & 1 & 0.8 \\ \hline
		$x_3$ &  & 0.96 & 0.8 & 1 \\ \hline
		\end{tabular}
	\caption{Таблица парных коэффициентов}
	\end{center}
\end{table}

Если все факторы тесно связаны, то в пределе все элементы матрицы равны единице и, как следствие, определитель данной матрицы будет равен нулю:
$$ \Delta =
\begin{vmatrix}
	1 & 1 & 1 \\
	1 & 1 & 1 \\
	1 & 1 & 1 \\
\end{vmatrix} = 0
$$

\textit{Критерий мультиколлинеарности:} если все элементы матрицы коллинеарности тесно связаны друг другом (факторы мультиколлинеарны), то определитель матрицы близок к нулю. 

Другой подход к оценке коллинеарности факторов - коэффициент множественной детерминации между факторами.

При наличии мультиколлинеарности факторов теряется смысл параметров линейной регресии, так как невозможно сделать правильный вывод или параметры модели (коэффициенты регресси) могут иметь неверные знаки и терять экономический смысл.

Рассмотрим на примере: между возрастом и стажем функциональная связь $\Rightarrow$ модель линейной регрессии нелогична.

При построении модели множественной регресии возможно использование разных математических функций.
Наиболее широко распространены две функции:

\begin{itemize}
	\item линейная $y = a + b_1x_1 + b_2x_2 $
	\item степенная $y = ax_1^b $
\end{itemize}

так как в этих функциях параметры имеют экономическую интерпретацию.

В линейных функциях переменные при параметрах называются \textit{коэффициентами регрессии}. В степенной зависимости параметры при $x$ выступают коэффициентами эластичности.

\subsubsection{Оценка параметров множественной линейной регрессии}

Рассмотрим модель вида:
$$ \widehat{y} = a +b_1x_1 + b_2x_2 + \ldots + b_px_p$$

Для оценки парамтров используется метод наименьших квадратов. Сумма остатков должна быть минимальной, система нормальных уравнений будет включать в себя n параметра. 
$$\left \{
\begin{matrix}
	\sum y = n \cdot a + b_1 \sum x_1 + b_2 \sum x_2 + \ldots + b_p \sum x_p \\[0.3cm]
	\sum y \cdot x_1 = a \sum x_1 + b_1 \sum x_1^2 + b_2 \sum x_1x_2 + \ldots + b_p \sum x_p x_1 \\[0.3cm]
	......................................................................................... \\[0.3cm]
	\sum y \cdot x_p = a\sum x_p + b_1 \sum x_1x_p + b_2 \sum x_2x_p + \ldots + b_p \cdot \sum x_p^2
\end{matrix}
\right.
$$
Решаем данную систему методом определителей. Каждый параметр определяем как
$$ a =\frac{\Delta_a}{\Delta}, b_1 =\frac{\Delta_b}{\Delta}, \ldots , b_p = \frac{\Delta b_p}{\Delta} $$

где $\Delta$ - определитель системы, а $\Delta_i:  i ={1,2,...,p}$ - частные определители.

При этом:
$$\Delta = 
\begin{vmatrix}
n & \sum x_1 & \sum x_2 & \ldots & \sum x_p \\[0.3cm]
\sum x_1 & \sum x_1^2 & \sum x_2x_1 & \ldots & \sum x_px_1 \\[0.3cm]
\sum x_2 & \sum x_1x_2& \sum x_2^2 & \ldots & \sum x_p x_2 \\[0.3cm]
\ldots & \ldots & \ldots & \ldots & \ldots \\[0.3cm]
\sum x_p & \sum x_1x_p & \sum x_2x_p & \ldots & \sum x_p^2
\end{vmatrix}$$

а $\Delta a, \Delta b_1, \ldots , \Delta b_p$ получаются путем замены соответствующего столбца матрицы определителя системы данными левой части системы.

В частности для модели от двух переменных получаем следующую модель:
$$ \widehat{y} = a +b_1x_1 + b_2x_2$$

И систему линенйных уравнений:
$$\left \{
\begin{matrix}
	\sum y = n \cdot a + b_1 \sum x_1 + b_2 \sum x_2 \\[0.3cm]
	\sum y \cdot x_1 = a \sum x_1 + b_1 \sum x_1^2 + b_2 \sum x_1x_2\\[0.3cm]
	\sum y \cdot x_2 = a\sum x_2 + b_1 \sum x_1x_2 + b_2 \cdot \sum x_2^2
\end{matrix}
\right.
$$

Коэффициенты регрессии $b_1$ и $b_2$ не сравнимы между собой, т.е, если, не умоляя общности, $b_1>b_2$ это не означает, что $x_1$ воздействует на целевую переменную больше, чем $x_2$.

Для оценки сранвительной силы факторов на результат строится регрессия в стандратизованном виде. 

При двух факторах $x_1,x_2$ строится регрессия вида:
$$ \hat{t_Y} = \beta_1t_{x_1} + \beta_2t_{x_2} $$
$$t_{Y} = \frac{y_i-\bar{y}}{\sigma_y}, t_{x_1} = \frac{x_{1i}-\bar{x_1}}{\sigma_{x_1}}, t_{x_2} = \frac{x_{2i}-\bar{x_2}}{\sigma_{x_2}}$$
строятся стандратизованные переменные. 

Среднее значение стандартизированной переменной 0, а средне квадратичное отклонение равно единице. Параметр $a=0$ , так как все средние равны нулю. 

К данному уравнению применяется метод наименьших квадратов.
$$ \sum_{i} (t_{y} - \bar{t_y})^2 = \sum_i (t_y - \beta_1t_{x_1} - \beta_2 t_{x_2})^2 \to \min $$

В данной системе:
$$\sum t_y t_{x_1} = n r_{yx_1} $$
$$\sum t_{x1}^2 = \sum t_{x2}^2 $$

$$\left \{
\begin{matrix}
r_{yx_1} = \beta_1 + \beta_2r_{x_1 x_2} \\
r_{yx_2} = \beta_1r_{x_1 x_2} + \beta_2 \\
\end{matrix}
\right.
$$

С ростом фактора $x_1$ на одну $\sigma$ результат $y$ возрастает на свою сигму
Если $\beta_1 > \beta_2$, то коэффициент влияет больше, чем.

Стандратизованные коэффициенты регресси связаны со стандартизованными переменными регресии:
$$ b_1 = \beta_1 \frac{\sigma_y}{\sigma_{x_1}}$$
$$ b_2 = \beta_1 \frac{\sigma_y}{\sigma_{x_2}}$$
$$ b_j = \beta_j \frac{\sigma_y}{\sigma_{x_j}}$$

Стандартизованный коэффициент регрессии связаны с коэффициентами эластичности, которые также показывают сравнительную силу влияния фактора на результат.
$$ \overline{\xi}_{y_{x_1}} = b_1 \frac{\overline{x_1}}{\overline{y}}$$
$$ \overline{\xi}_{y_{x_2}} = b_2 \frac{\overline{x_2}}{\overline{y}} $$

Коэффициент вариации - сколько составляет отклонение от среднего уровня.
$$ k_v = \frac{\overline{\xi}}{\sigma} $$
МНК для нелинейной множественной регресии применяется, если нелинейная функция преобразована в линейную (аналогично с парной регрессией).

\subsubsection{Показатели силы зависимости множественной регрессии}
Основным показателем корреляции является индекс корреляции:
$$ R =\sqrt{\frac{SS_R}{SS_T}} = \sqrt{1 - \frac{SS_E}{SS_T}} = \sqrt {1 - \frac{\sum (Y - \hat{Y_i})^2}{\sum (Y - \overline{Y})^2}} \eqno {43}$$
$$ 0 \leq R \leq 1 $$

Только при линейной регрессии коэффициент детерминации определяется по формуле:
$$ R^2 = \sum_{i=1}^{k} r_{yx_i}\beta_i$$

Показатель множественной корреляции всегда должен быть больше (либо равен) максимальному парному коэффициенту корреляции:
$$R_{yx_1x_2\ldots x_n} \geq  r_{yx_i \max} $$

Коэффициент детерминации может быть определн через матрица парных коэффициентов корреляции:
$$ R^2 = 1 -\frac{| \Delta r |}{\Delta r_{11}} $$

где $\Delta r$ - определитель матрицы парных коэффициентов корреляции, а
$\Delta r_{11}$ - определитель матрицы межфакторной корреляции.

При расчете коэффициента множественной корреляции используется остаточная дисперсия, которая имеет систематическую ошибку в сторону преуменьшения, чем больше параметров определяется в модели при заданном объеме $n$. Значение $R^2$ увеличивается от добавления в модель новых переменных, даже если эти переменные никакого отношения к объясняемой переменной не имеют. Поэтому принятно считать \textit{скорректированный} (нормированный) коэффициент множественной корреляции, учитывающий число степеней свободы.
$$ R = \sqrt{1 - \frac{MS_E}{MS_T}}$$
$$ \hat{R^2} = 1 -(1-R^2) \frac{n-1}{n-m-1}$$

Данный коэффициент зависит от объема наблюдения и числа параметров расчитываемой модели. Чем больше $n$, тем меньше коэффициент детерминации и скорректированный коэффициент детерминации различаются.

Если МНК используется к преобразованным $Y$, то определяется \textit{квази-$R^2$}.

\subsubsection{Частная корреляция для нелинейной регресии}

В множественной регресии кроме общего показателя совокупной корреляции расчитывается показатель частной корреляции. Она должна показать в чистом виде влияние факторов на результат при устранении влияние других факторов, включенных в уравнение регрессии.

Метод сокращения остаточной корреляции.
Рекурентая формула (матрица парных коэффициентов корреляции).

Предположим, что рассматривается модель:
$$y=a +b_1x_1 + \ldots + b_px_p + \varepsilon $$

Частная корреляция может быть разного порядка. Порядок частной корреляции зависит от того, сколько факторов закрепляется на постоянном уровне. 
\begin{itemize}
	\item Если закрепляется один фактор $R_{yx_1 | x_2}$ - частная корреляция первого порядка при постоянном $x_2$
	\item Если закрепляется два фактор $R_{yx_1 | x_2x_3}$ - частная корреляция второго порядка при постоянном $x_2,x_3$
\end{itemize}

Частная корреляция может быть определена как отношение сокращения остаточной дисперсии за счет дополнительного включения в анализ нового фактора к остаточной дисперсии, имевшей место до введения данного фактора в модель.

\textbf{Пример:}

Предположим, что изучается зависимость $x_1$ и $x_2$. Сначала было построено уравнение регресии $y(x_1)$.
$$Y = 27.5 +3.5x_1, S_{y|x_1}^2 = 6$$

Далее был добавлен новый фактор:
$$ Y = 20.2 +2.8x_1+ 0.3x_2, S_{y|x_1x_2}^2 = 3.7 $$

Тогда сокращение остаточной дисперсии:
$$S_{yx_1}^2  - S_{y|x_1x_2}^2  = 2.3$$

Доля сокращения остаточной дисперсии:
$$\frac{S_{yx_1}^2  - S_{y|x_1x_2}^2}{S_{y|x_1}^2}  = 0.383$$

чем данная доля больше, тем теснее связаны между собой $y$ и $x_2$ при постоянном действии фактора $x_1$.

Индекс частной корреляции вычисляется по формуле:
$$ R = \sqrt{\frac{S_{y|x_1}^2  - S_{y|x_1x_2}^2}{S_{y|x_1}^2}} \eqno \blacksquare$$

При двухфакторной модели:
$$ \hat{y} = a +b_1x_1 + b_2x_2$$

Частная корреляция равна:
$$ r_{yx_1x_2} = \sqrt{1 - \frac{1 - R_{yx_1x_2}^2}{1-r_{yx_2}^2}}$$
$$ r_{yx_2x_1} = \sqrt{1 - \frac{1 - R_{yx_1x_2}^2}{1-r_{yx_1}^2}}$$

Возможно использовать при расчете корреляции рекуррентные формулы:
$$r_{yx_1x_2} = \frac{r_{yx_1} -r_{yx_2}\cdot r_{x_1x_2} }{\sqrt{(1-r_{yx_2}^2)\cdot (1-r_{x_1x_2}^2)}} $$
$$r_{yx_2x_1} = \frac{r_{yx_2} -r_{yx_1}\cdot r_{x_1x_2} }{\sqrt{(1-r_{yx_1}^2)\cdot (1-r_{x_1x_2}^2)}} $$

Коэффициенты частной корреляции более высоких порядков рассчитываются через коэффициенты корреляции более низких порядков.

Частная корреляция используется в эконометрике для отбора факторов (метод отсева).
$$ t_y = \beta_1 t_{x_1} + \beta_2 t_{x_2} + \beta_3 t_{x_3}$$


Частные коэффициенты корреляции связаны со стандартизованными коэффициентами регрессии. То есть если:
$$ \beta_1 \geq \beta_2 \geq \beta_3 $$

то:
$$
r_{yx_1|x_2x_3} \geq r_{yx_2|x_1x_3} \geq  r_{yx_3|x_1x_2} 
$$

\subsubsection{Оценка значимости модели множественной регрессии}
Дается, как и для парной регрессии, с помощью $F$-критерия: (20) и (23).

В множественной регрессии может быть посчитан \textit{частный F-критерий}, который оценивает целесообразность включения фактора в модель последним, т.е при закреплении других факторов на постоянном уровне.

Формула для расчета частного F-критерия - прирост факторной дисперсии, обусловленного включением данного фактора в модель, в сравнении с остаточной дисперсии на одну степень свободы по регрессионной модели в целом.

При двуфакторной модели:

$$F_{x_1} = \frac{R_{yx_1x_2}^2 - r_{yx_2}^2}{1-R_{yx_1x_2}^2} \cdot (n-3)$$
$$F_{x_2} = \frac{R_{yx_1x_2}^2 - r_{yx_1}^2}{1-R_{yx_1x_2}^2} \cdot (n-3)$$

Частный $F$-критерий связан с $t$-критерием Стьюдента для соответствующего фактора:
$$ t_{b_i} = \sqrt{F_{x_i}}$$

\subsubsection{Использование множественной регрессии для прогноза}

В начале путем подстановки точных значений факторов, определяется точечный прогноз. Далее, может быть дан интервальный прогноз. С этой целью определяется \textit{предельная ошибка} прогнозируемого значения $y$.
$$ \delta_{\hat{y}_p} = t_{table} m_{\hat{y}_p}$$
Средняя ошибка прогноза:
$$m_{\hat{y}_p}  = \sqrt{\frac{\sum \xi_i^2}{n-m-1} (1+X_p^T(X^T X)^{-1}X_p)}$$
Стандартная ошибка регрессии:
$$S =\sqrt{\frac{\sum \xi_i^2}{n-m-1}}$$

\subsubsection{Анализ остатков регрессии}

После получения уравнения регрессии проводится анализ остатков, чтобы оценить, насколько достоверна регрессионная модель.

Для оценки параметров используется МНК. Применение МНК предполагает, что полученные оценки параметров состоятельны, несмещенные и эффективные.

\textit{Состоятельность} - оценка состоятельна, если при увеличении объема выборки стремится к оцениваемому параметру. Увеличивается точность при увеличении наблюдения.

Для \textit{несмещненных} оценок показатели точности оценки служит дисперсия - чем дисперсия меньше, тем оценка лучше. При росте $n$ ошибки не накапливаются и являются несмещенными при малой дисперсии.

\textit{Эффективность} - имеет минимальную диперсию среди всех несмещенных оценок.

Свойства остатков:

\begin{itemize}
	\item Отсутствие связи между остатками о объясняюзей переменной
	\item Отсутствие связи между остатками и предсказанными значениями
	\item Математическое ожидание остатков равно нулю
	\item Остатки имеют постоянную диперсию Диперсия остатков равна единица. Постоянство дисперсии - гомоскедастичность. Гетероскедастичность - диперсия остатков непостоянна.
	\item Остатки не коррелированы между собой
	\item Остатки распределны по нормальному закону.
\end{itemize}

Предпосылки МНК:
\begin{itemize}
	\item случайный характер остатков
	\item $\mathbb{E} \xi = 0$: не зависят от $x$.
	\item Гомоскедастичность остатков - дисперсия каждого остатка одинакова для каждого $x$.
	\item Отсутствие автокорреляции
	\item Остатки подчиняются нормальному распределению.
\end{itemize}

Автокорреляция остатков - каждый последующий остаток не должен зависеть от предыдущего остатка. Линейный коэффициент корреляции - автокорреляция.

Тесты и критерии гетероскедастичности: тест ранговой корреляции Спирмена. Остатки рассматриваются по модулю, устанавливается их зависимость от значений фактора $x$. Если $|e_i|$ и $x_i$ будут коррелированы, то делается вывод о гетероскедатичности.
$$ R = 1 - \frac{6\sum d^2}{n \cdot (n^2 - 1)}$$
$$t_R < t_{\alpha}$$

Тест Гольдфера - Квандта:

Наличие гетероскедастичности. Сначала упорядочивается по значению $x_i$, упорядоченная выборка разбивается на три группы: центральная, ниже центральной, выше центральной.

Оцениваются отдельные ркоэффициенты регрессии для первой подвыборки (k первых наблюдений) и для третьей подвыборки (k последних наблюдений). Если предпололжение о пропорциональности дисперсий отклонений значениям $x_j $ верно, о $\sum e^2$/.

Проверка нулевой гипотезы:

$$ F = \frac{S3}{n-m-1} \ \frac{S1}{n-m-1}$$
которая при сраведливости нуль-гипотезы имеет распределение Фишера(k-m-1,k-m-1) степенями свободы.
Если $F_{fact} >F_{table}$, то имеет гетероскедастичность в остатке.

Объем исключаемых данных равен 3/4.

Количественная оценка гетероскедастичности обычно оценивается с помощью тестов Уайта, Парка, Глейзера.
Данные тесты предполагают, что дисперсия рассматрвиается как функция некоторых факторов.

Тест Парка:
$$\ln e_i^2 = a +b\ln x_{ij}+v_i$$, где $x_{ij}$ - i-ое значение $j$-ого фактора, $v_i$ - случайный остаток.

Тест Глейзера:
$$ |e_i| = a + bx_{ij}^k + v_i, k = -1, \ldots$$

Тест Уайта позволяет оценить количестевнно зависимость дисперсии ошибок регрессии.

ОМНК - если гетероскедастичность не удалось убрать с помощью изменения спецификации модели, то можно убрать. Обобщенный метод наименьших квадратов применяется к преобразованным данным.

Для устранения гетероскедастичности высказываются предположения о концепции поведения дисперсии. Предполгается что дисперсия остатков пропорционалана значению фактора $x$ с каким-либо коэффициентом $k$, то есть $\sigma_{ei}^2 = \sigma^2K_j$

Пример: выдвигаем гипотезу, что дисперсия ошибок пропорциональна $x^2$. $Y=a+bX+eX$. Убедиться в верности такой гипотезы можно применить тест Уайта.

$$\frac{y}{x} = \frac{a}{x} + b + e$$
$$Y = b +\frac{a}{x} + e$$

Параметры $a,b$ поменялись местами - константа стада коэффициентов наклона линии регрессии, а коэффициент регресии - свободным членом.
Применим МНК:
$$ S = \sum \frac{1}{K_i}(y_i - a - bx_i)^2$$
\end{document}

