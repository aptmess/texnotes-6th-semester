\RequirePackage{ifluatex}
\let\ifluatex\relax

\documentclass[aps,%
12pt,%
final,%
oneside,
onecolumn,%
musixtex, %
superscriptaddress,%
centertags]{article} %% 
\topmargin=-40pt
\textheight=650pt
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
%всякие настройки по желанию%
\usepackage[colorlinks=true,linkcolor=black,unicode=true]{hyperref}
\usepackage{euscript}
\usepackage{supertabular}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb, amsmath}
\usepackage{textcomp}
\usepackage[noend]{algorithmic}
\usepackage[ruled]{algorithm}
\usepackage{lipsum}
\usepackage{indentfirst}
\usepackage{babel}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}

\pgfplotsset{model/.style = {blue, samples = 100}}
\pgfplotsset{experiment/.style = {red}}

\selectlanguage{russian}

\setlength{\parindent}{2.4em}
\setlength{\parskip}{0.1em}
%\renewcommand{\baselinestretch}{2.0}

\usepackage{xcolor}
\usepackage{hyperref}
 
 % Цвета для гиперссылок
%\definecolor{linkcolor}{HTML}{799B03} % цвет ссылок
%\definecolor{urlcolor}{HTML}{799B03} % цвет гиперссылок
 
%\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\begin{document}

\begin{titlepage} 
\begin{center}
% Upper part of the page
%\textbf{\Large САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ ЭКОНОМИЧЕСКИЙ УНИВЕРСИТЕТ} \\[1.0cm]
%\textbf{\large Кафедра Прикладной Математики и Информатики}\\[3.5cm]
 
% Title
\textbf{}\\[10.0cm]
\textbf{\LARGE Эконометрика}\\[0.5cm]
\textbf{\Large ПМ-1701} \\[0.1cm]

%supervisor
\begin{center} \large
{Преподаватель:} \\[0.5cm]
\textsc {Курышева Светлана Владимировна }\\
%{viktor\_chernov@mail.ru}\\
\end{center}
% \begin{flushright} \large
%\emph{Рецензент:} \\
%д.ф. - м.н., профессор \textsc{Надеемся Нам Помогут}
%\end{flushright}
%\begin{flushright} \large
%\emph{Заведующий кафедрой:} \\
%д.ф. - м.н., профессор \textsc{Не Обмани Себя}
%\end{flushright}
\vfill 

% Bottom of the page
{\large {Санкт-Петербург}} \par
{\large {2020 г., 6 семестр}}
\end{center} 
\end{titlepage}

% Table of contents
\begin{thebibliography}{3}
\bibitem{eliseeva}
Эконометрика: Учебник/И.И.Елисеева и др.-М.:Проспект, 2009
\bibitem{praktikuma}
Практикум по эконометрике: Учебное пособие/И.И.Елисеева и др.,М.:Финансы и статистика,2006 
\bibitem{dop1}
Эконометрика: Учебник/В. С.Мхитарян и др.-М.:2008
\bibitem{dop2}
Доугерти К. Введение в эконометрику: Учебник. 2-е изд. / Пер. с англ. – М.: ИНФРА – М, 2007
\bibitem{dop3}
Берндт Э. Практика эконометрики: классика и современность. М.,2005
\end{thebibliography}
\tableofcontents
\newpage
\section{07.02.2020}
\subsection{Общие понятия об эконометрике} 
\textbf{Эконометрика} - это наука, которая дает конкректное количественное выражение закономерностям и взаимосвязям экономических явлений и процессов с помощью статистико-математических методов и моделей.

\underline{Связь эконометрики с другими науками:}
\begin{itemize} 
  \item Экономическая теория (сущность связи явлений)
  \item Статистика (информационная база)
  \item Математические и статистические методы:
  \begin{itemize} 
  	\item $C=k\cdot Y+L$, $0<|k|<1$ - регрессия
  	\item $r=sC+Dx+T$, где $t$ - сбережения, а $x$ - инвестиции
  	\item Если s=D, то $t=s(C+x)+T$  - уравнение двухфакторной регрессии
  	\item $Y+r=C+x$ - балансовое тождество 
  \end{itemize} 
\end{itemize}

\underline{Этапы построения эконометрической модели:}
\begin{enumerate}
	
	\item Теоретоическое описание рассмариваемого процесса
	\item Сбор данных, анализ их качества
	\item Спецификация модели
	\begin{enumerate}
	\item Выявление объясняемых ($Y$) и объясняющих ($X$) переменных 
	\item Выбор функций
	\end{enumerate}
	\item Оценка параметров модели
	\item Верификация модели (т.е проверка достоверности)
	\item Интерпертация результатов 

\end{enumerate}

\subsection{Парная регрессия и корреляция в эконометрических исследованиях}
\underline{Последовательность анализа регрессии:}
\begin{enumerate}
	\item Выбор типа математической функции при построении уравнения регрессии
	\item Оценка параметров уравнения
	\item Показатели силы связи
	\item Статистическая оценка достоверности ($F$-критерий Фишера)
	\item Интвервальная оценка параметров уравнений парной регрессии
	\item Использование модели
\end{enumerate}

\underline{Выбор функции для модели может проводиться $3$-мя способами}
\begin{enumerate}
	\item Аналитический
	\item Графический
	\item Экспериментальный
\end{enumerate}

\underline{Основные виды функций в модели парной регресии:} \\ 
$y=ax+b, y=a+\frac{b}{x}, y=a+bx+cx^2,y=ax^b, a=b^x, y=ae^{bx} $
\subsection{Предпосылки регрессионной модели}
1. Модель линейна по параметрам

2. $\mathbb{E}\xi_i = 0 \forall i$, т.е ожидание значения случайного члена должно быть равно нулю в каждом наблюдении из-за того, что каждое наблюдение не должно включать в себя смещения ни в каком из направлений.

3. $\mathbb{D}\xi_i = Const $, т.е его значение в каждом наблюдении получено из распределения с постоянной теоретической дисперсией. Также не должно быть причин , делающих его больше подверженным ошибке в одних наблюдениях по сравнению с другим. Заметим, что $$\mathbb{E}\xi_i^2 = \mathbb{D}\xi_i = \mathbb{D}\sigma_{\xi_i}^2 | \forall i $$

4. Значения случайного члена имеют взаимно независимые распределения. Случайный член не подвержен автокорреляции, т.е отсутствует систематическая связь между его значениями в любых двух наблюдениях.
Ковариация равна нулю:
$$ \sigma_{\xi_{i}\xi_{j}} = \mathbb{E}(\xi_i\xi_j) = \mathbb{E}\xi_i \cdot \mathbb{E}\xi_j = 0 | \forall i \neq j $$

5. $\xi_i \sim \mathbb{N}(0,\sigma^2)$: если случайный член нормально распределен, то распределены нормально и коэффициенты регрессии.

\subsection{Оценка параметров модели}
Рассмотрим случаи, для которых мы хотим предпооложить, что одна \textit{зависимая} переменная $Y$ определяется другими переменными, называемые \textit{объясняющими} переменными (регрессорами). Математическая зависимость, связывающая эти переменные, называется \textit{моделью регрессии}. Мы допускаем, что модель регрессии имеет факт неточности - \textit{случайный} (остаточный) член.

Начнем с рассмотрения простешей модели:
$$ Y_i = \alpha + \beta X_i + \xi_i \eqno (1) $$

$Y_i$ - значение зависимой переменной, $\alpha$ и $\beta$ - постоянные величины - параметры уравнения, $\xi_i$ - случайный член.

Задача регрессионого анализа состоит в получении оценок $\alpha$ и $\beta$ и, следовательно, в определении положения прямой по точкам $\Leftrightarrow$ нужно посроить прямую, в наибольшей степени соответствующую этим точкам.

$a$ - отсечение $Y$ - оценка $\alpha$

$b$ - угловой коэффициент  - оценка $\beta$

Пусть $$\hat{Y} = a+bX_i \eqno (2)$$ 
оцениваемая модель, а $Y_i$ - оцененное значение $Y$. Наша задача заключается в том, чтобы выяснить, существут ли способы оценки коэффициентов $a,b$ алгебраическим путем.

Обозначим за $$e_i = Y_i - \hat{Y_i} =Y_i - a - bX_i \eqno (3)$$

Остаток наблюдений зависит от выбора коэффициентов $a$ и $b$ $\Rightarrow$ задача заключается в том, чтобы выбрать такие $a$ и $b$, предсказанное значение функции от искомой в каждой точке было минимальным. Глупо минимизировать сумма остатков, потому что при выборе выборочного среднего модели: $$\sum e_i = 0 \eqno (4)$$ 

Поэтому будем минимизировать сумму квадратов остатков. Данный метод называется \textit{Методом Наименьших Квадратов } или сокращенно МНК.
\subsubsection {Метод наименьших квадратов}

Пусть у нас имеются $n$ наблюдений $(X_i,Y_i)$, $Y$ зависит от $X$ и мы хотим подобрать уравнение: $$\hat{Y} = a+bX_i$$

Запишем формально нашу задачу в обозначениях метода наименьших квадратов (МНК):

$$ S = \sum (Y_i-\hat{Y_i})^2 = \sum (Y_i - a - bX_i)^2 \to min \eqno (5) $$
$$\left\{
\begin{matrix}
\frac{\partial S }{\partial a} = -2 \sum Y + 2na+2b\sum X = 0 \\

\frac{\partial S }{\partial b} = -2 \sum YX + 2a\sum X+2b\sum X^2 = 0 
\end{matrix} \right. \eqno (6)$$

Применение метода наименьших квадрато приводит к системе уравнений, которая для линейных уравнений имеет вид: \\
$$ \left\{
\begin{matrix}
\sum Y = na + b\sum X \\
\sum YX = a\sum X + b\sum X^2 \\
\end{matrix} \right. \eqno (7) $$

Решим данную систему линенйных уравнений методом Крамера:
$$ \delta =
\begin{vmatrix}
n & \sum X \\
\sum X & \sum X^2\\
\end{vmatrix}
= n\cdot \sum X^2 - (\sum X)^2 $$

$$ \delta_{b} =
\begin{vmatrix}
n & \sum Y \\
\sum X & \sum XY\\
\end{vmatrix} = n\cdot \sum XY - \sum X\sum Y $$
$$ b = \frac{\delta_{b}}{\delta} = \frac{n\cdot \sum XY - \sum X\sum Y}{n\cdot \sum X^2 - (\sum X)^2} = \frac{ \frac{\sum XY}{n} -\frac {\sum X\sum Y}{n}}{\frac{\sum X^2}{n} - \frac{(\sum X)^2}{n^2}} = \frac {\overline{XY} - \overline{X} \cdot \overline{Y}}{\overline{X^2} - (\overline{X})^2} = \frac {cov(x,y)}{\sigma_x^2} $$

Итого получаем коэффициенты предполагаемой модели:
$$ b = \frac {cov(x,y)}{\sigma_x^2}  = \frac{\sum (X_i - \overline{X})\cdot (Y_i - \overline{Y})}{\sum (X_i-\overline{X})^2} = \frac {\overline{XY} - \overline{X} \cdot \overline{Y}}{\overline{X^2} - (\overline{X})^2} \eqno (8) $$
$$ a = \overline{Y} - b\overline{X} \eqno (9) $$

$b$ - наклон линии регресии (коэффициент регрессии) - абсолютный показатель силы связи.

Свойства метода МНК (результаты относительно регрессий, оцениваемых по обычному МНК):

\begin{enumerate}
	\item $\sum e_i = 0$
	\item $\overline{e} = 0$
	\item $ \overline{\hat{Y}} = \overline{Y}$
	\item $\sum X_i \cdot e_i = 0$
	\item $\sum \hat{Y_i} \cdot e_i = 0$
\end{enumerate}

Уравнение регрессии всегда дополняется обязательным показателем тесноты связи. При использовании линейной регресии в качестве такого показателя выступает \textit{линеный коэффициент корреляции $r_{xy}$}.
Существует разные модификации формулы линейного коэффициента корреляции:
$$
r = b \frac{\sigma_x}{\sigma_y} =\frac{cov(x,y)}{\sigma_x^2} \cdot \frac{\sigma_x}{\sigma_y} =  \frac{\overline{YX} - \overline{Y} \cdot \overline{X}}{\sigma_x \sigma_y}, -1 \leq r \leq 1 \eqno (10)
$$

Шкала значений коэффициента корреляции (все значения берутся по модулю):
\begin{itemize}
	\item $r \leq0.3$ - связь слабая
	\item $0.3 < r \leq 0.5$ - связь умеренная
	\item $0.5 < r \leq 0.7$ - связь заметная
	\item $0.7 < r \leq 0.9$ - связь высокая
	\item $0.9 < r \leq 1$ - связь весьма высокая, близкая к функциональной
\end{itemize}

Следует иметь ввиду, что величина линейного коэффициента корреляции оценивает тесноту связи рассматриваемых признаков в её линейной форме. Поэтому близость абсолютной величины линейного коэффициента корреляции к нулю \textit{еще не означает отсутствие связи} между признаками.

Для оценки качества подбора линейной функции расчитывается квадрат линейного коэффициента корреляции $r_{yx}^2$, называемый \textbf{коэффициентом детерминации}. Коэффициент детерминации характеризует долю дисперсии результативного признака $y$, объясняемую регрессией, в общей дисперсии результативного признака.
$$r_{yx}^2 = \frac{\sigma_{y,obyasn}^2}{\sigma_{y,obch}^2} \eqno (11)$$

\subsubsection{Качество модели: коэффициент детерминации}
Цель регрессии - объяснение поведения $Y$. В любой выборке $Y$ оказываетяс низким, а в других - высоким. Разброс значений $Y$ можно описать с помощью суммы квадратов отклонений от выборочного среднего.
$$ \sum (Y - \overline{Y})^2 $$

Все показатели корреляции основаны на правиле сложения дисперсий $\Rightarrow$ можно разложить \textbf{общую сумму квадратов отклонений } переменной $Y$ от среднего значения $\overline{Y}$ на две части - \textbf{"объясненную" } сумму квадратов и \textbf{"необъясненную"}. 
$$\sum (Y - \overline{Y})^2 = \sum (\hat{Y}-\overline{Y})^2 + \sum (Y - \hat{Y})^2 \eqno (12)$$

Данное равенство можно переписать как:
$$SS_T = SS_R + SS_E \eqno (13)$$

где: 

$SS_T  = \sum (Y - \overline{Y})^2 $  - общая сумма квадратов отклонений \textit{(total sum of squares)}

$SS_R = \sum (\hat{Y}-\overline{Y})^2 $ - \textbf{сумма квадратов отклонений, объясненная} регрессией, \textbf{факторная сумма} \textit{(sum of square due to regression)}

$SS_E = \sum (Y - \hat{Y})^2  = \sum e_i^2 $ - \textbf{остаточная сумма} квадратов отклонений,
\textit{(sum of square due to error)}.

Введем \textbf{коэффициент детерминации}:
$$ R^2 = r^2 = \frac{\sigma_{y,obyasn}^2}{\sigma_{y,obch}^2} = \frac{SS_R}{SS_T} = 1 - \frac{SS_E}{SS_T} = 1 - \frac{\sum (Y - \hat{Y})^2}{\sum (Y - \overline{Y})^2} $$
$$ R^2 = \frac{\sum (\hat{Y}-\overline{Y})^2}{\sum (Y - \overline{Y})^2} = 1 - \frac{\sum (Y - \hat{Y})^2}{\sum (Y - \overline{Y})^2} \eqno (14) $$

\textbf{Коэффициент детерминации}- обобщающий показатель оценки качества построенного уравнения регрессии.
\subsubsection{Статистическая оценка достоверности регрессионной модели}

После того как найдено уравнение линейной регресии, проводится оценка значимости как уравнения в целом, так и отдельных его параметров. Оценка значимости уравнения регрессии в целом дается с помощью $F$-критерия Фишера. При этом выдвигается нулевая гипотеза, что коэффициент регрессии равен нулю, т.е $b=0$ и, следовательно, фактор $x$ не оказывает влияния на результат $Y$.

Выберем нулевую гипотезу, по которой мы будем оценивать качество модели (в генеральной совокупности):
\begin{center}
	$H_0$: $r^2 = 0 $  \\
	$H_1$: $r^2 \neq 0 $ 
\end{center}

Если же прочие факторы не влияют на результат, то $Y$ связан с $X$ функционально и остаточная сумма квадратов $SS_E = \sum e_i^2 = 0$. В этом случае сумма квадратов отклонений равна объясненной сумме квадратов: $$SS_T = SS_R$$

Поскольку не все точки поля корреляции лежат на линии регрессии, то всегда имеет место их разброс как обусловленный влиянием фактора $X$, т.е регрессией $Y$ по $X$, так и вызванный действием прочих причин(необъясненная вариация). 

Так как  $$ R^2 = 1 - \frac{\sum (Y - \hat{Y})^2}{\sum (Y - \overline{Y})^2}  = 1 - \frac{SS_E}{SS_T},$$ 

то если $SS_T$ будет больше остаточной суммы квадратов $SS_E$, то уравнение регрессии статистически значимо и фактор $X$ оказывает существенное воздействие на результат $Y$. Это равносильно тому, что коэффициент детерминации $R^2$ будет приближаться к единице.

Любая сумма квадратов отклонений связана с числом степени свободы (\textit{df - degrees of freedom}), т.е числом свободы независимого варьирования признака. Число степеней свободы связано с числом единиц совокупности $n$ и с числом определяемых по ней констант.

При расчете объясненной или факторной суммы квадратов $\sum (\hat{Y} - \overline{Y})^2$ используются теоретические (расчетные) значения результативного признака $\hat{Y}$, найденные по линии регресии: $$\hat{Y} = a+bX_i$$ 

Сумма квадратов отклонений, обусловленных линейной регрессией (следует из формулы линейного коэффициента корреляции): 
\label{SSR}
$$ SS_R = \sum (\hat{Y_i}-\overline{Y})^2 = b^2 \cdot \sum(X-\overline{X})^2 \eqno (15)$$

так как по формулам (11) и (14):

$$ r_{yx}^2 = \frac{\sigma_{y,obyasn}^2}{\sigma_{y,obch}^2} = \frac{SS_R}{SS_T}=\frac{\sum (\hat{Y}-\overline{Y})^2}{\sum (Y - \overline{Y})^2} = b^2 \cdot \frac{\sigma_x^2}{\sigma_y^2}  
$$
$$\sum (\hat{Y}-\overline{Y})^2 = b^2 \cdot \frac{\sigma_x^2}{\sigma_y^2} \cdot \sum (Y - \overline{Y})^2 =b^2 \cdot \frac{\sum (X - \overline{X})^2}{\sum (Y - \overline{Y})^2} \cdot \sum (Y - \overline{Y})^2 = b^2 \cdot \sum (X - \overline{X})^2 
$$

Данная сумма квадратов отклонений имеет 1 степень свободы, так как зависит только от одной константы коэффициента регрессии $b$, следовательно: $$df_{SS_R} = 1 $$

Число степеней свободы остаточной суммы квадратов при линейной регрессии: $$df_{SS_E}=n-2$$

Число степеней свободы для общей суммы квадратов определяется числом единиц, и поскольку мы используем среднюю вычисленную по данным выборки, то теряем одну степень свободы, следовательно: $$df_{SS_T} = n-1$$

В случае линейной регрессии получаем следующее равенство: $$ n-1 = 1 + (n-2)$$

В общем случае: $$ df_{SS_T} = df_{SS_R} + df_{SS_E}$$ $$ n-1 = m + ( n - 1 - m) $$ 
$$df_{SS_T}=n-1, df_{SS_R} = m, df_{SS_E} = n-1-m \eqno (16)$$
где $m$ - число параметров переменных.

Разделив каждую сумму квадратов на соответствующее ей число степеней свободы, получим средний квадрат отклонений или \textbf{дисперсию на одну степень свободы}:

$$ MS_R = \frac {SS_R} {df_R} = \frac{\sum (\hat{Y}-\overline{Y})^2} {m} \eqno (17)$$
$$ MS_E = \frac {SS_E} {df_E} = \frac{\sum (Y - \hat{Y})^2} {n - 1 - m} \eqno (18)$$
$$ MS_T = \frac {SS_T} {df_T} = \frac{\sum (Y - \overline{Y})^2}{n-1}\eqno (19) $$
где $MS_T$ - общая дисперсия, $MS_E$ - остаточная, $MS_R$ - факторная (объясненная).

Определение дисперсии на одну степень свободы приводит дисперсии к \textit{сравнимому виду}. Сопоставляя факторную (объясненную) и остаточную дисперсию в расчете на одну степень свободы, получим величину \textbf{$F$-критерия}:
\label{Snedekor}
$$F=\frac {MS_R} {MS_E } =  \frac {\text{Factor Variance with 1 df} } {\text{Remainder variance with 1 df}} \eqno (20)$$

Значение $F_{table}$ означает максимальную величину отношения дисперсия при случайном их расхождении для данного уровня вероятности и наличия нулевой гипотезы.

В математической статистике данное распределение называется распределение \textit{Снедекора} для $(n,m)$ степеней свободы.

Для проверки гипотезы о значимости уравнения регресии воспользуемся следующим алгоритмом:
\begin{enumerate}
	\item Выберем в достоверной области критический уровень значимости $\alpha$. Обычно выбирают маленький уровень значимости, так как вероятность попадания в критическую область при справедливости нулевой гипотезы $H_0$ должна быть маленькой ($\alpha \approx 0.05$).
	\item Определяется табличное критическое значение критерия Фишера $F_{table}$
	\item Если $F>F_{table}$, то $H_0$ отвергается $\Rightarrow$ гипотеза о случайности природы отвергается и делается вывод о существенности связи и значимости $R^2$
\end{enumerate}

Если нулевая гипотеза справедлива, то факторная (объясненная) и остаточная дисперсия не отличаются друг от друга. 
Величиная $F$-критерия связана с коэффициентом детерминации $r^2$. Факторную сумму квадратов отклонений можно представить как:
$$ SS_R = \sum (\hat{Y_i}-\overline{Y})^2 = b^2 \cdot \sum(X-\overline{X})^2  = r_{yx}^2\cdot \sigma_y^2 \cdot n \eqno (21)$$

так как:
$$ SS_R = \sum (\hat{Y_i}-\overline{Y})^2 = r_{yx}^2 \cdot SS_T = r_{yx}^2 \sum (Y - \overline{Y})^2 =$$
$$ r_{yx}^2 \cdot \frac{1}{n} \sum (Y - \overline{Y})^2 \cdot n = r_{yx}^2 \cdot \sigma_{y}^2 \cdot n  $$

А остаточную сумму квадратов как:
$$ SS_E = \sum (Y_i-\hat{Y_i})^2 = (1-r_{yx}^2)\cdot \sigma_y^2 \cdot n \eqno (22)$$

так как:

$$ r_{xy}^2 = 1 - \frac{SS_E}{SS_T} \Rightarrow SS_E = SS_T \cdot (1-r_{xy}^2) = (1-r_{yx}^2)\cdot \sigma_y^2 \cdot n $$

Тогда значение $F$-критерия равно:
\label{FR}
$$F = \frac {MS_R} {MS_E } = \frac{\frac {SS_R} {df_R}}{\frac {SS_E} {df_E}} = \frac{r_{yx}^2}{(1-r_{yx}^2)} \cdot \frac{n-1-m}{m} \eqno (23)$$ 
где $n$ - число единиц в совокупности, $m$ - число параметров при переменных.

Результаты факторного анализа обычно представлены в таблице дисперсионного анализа
\label{first_table_analiz}
\begin{table}[H]
	\begin{center}
		\begin{tabular}[t]{|c|c|c|c|c|} \hline
		Источник вариации & df & $SS$ & $MS$ & F-критерий\\ \hline
		Регрессия & 1 & 14735 & 14735 & 278 \\ \hline
		Остаток & 5 & 265 & 53 & 1 \\ \hline
		Итого & 6 & 15000 & x & x \\ \hline
		\end{tabular}
	\caption{Таблица дисперсионного анализа для примера}
	\end{center}
\end{table}
$F_{table} = 6.61$, 278 > 6.61 - регресия статистически значима, $r^2 \neq 0$
\subsubsection{Оценка значимости коэффициентов регрессии} 

В линейной регресии обычно оценивается значимость не только уравнения в целом, но и отдельных его параметров. С этой целью по каждому из параметров строится его \textbf{стандартная ошибка} (случайная ошибка коэффициента регрессии).

Выдвигается нулевая гипотеза о равенстве коэффициентов регресии в генеральной совокупности:
$$H_0: b= 0 $$ 
$$H_1: b \neq 0 $$ 

\textbf{Стандартная ошибка} коэффициента регрессии определяется по формуле:
$$m_b= \sqrt {\frac{MS_E}{\sum (X-\overline{X})^2}} = \sqrt {\frac{\frac{\sum (Y - \hat{Y})^2} {n - 1 - m}}{\sum (X-\overline{X})^2}} \eqno (24)$$

Вводится t-статистика:
$$ t_{b}=\frac {b - 0}{m_b} = \frac{b}{m_b} \sim t(n-2) \eqno (25)$$

так как два параметра, то число степеней свободы равно двум и данная статистика имеет распределение Стьюдента с $n-2$ степенями свободы.

Для проверки гипотезы о значимости коэффициента регресии воспользуемся следующим алгоритмом:
\begin{enumerate}
	\item Выберем в достоверной области критический уровень значимости $\alpha$. Обычно выбирают маленький уровень значимости, так как вероятность попадания в критическую область при справедливости нулевой гипотезы $H_0$ должна быть маленькой. 
	\item Определяется табличное критическое значение критерия Стьюдентая $t_{table}(n-2)$
	\item Если $|t_{b}| > t_{table}$, то $H_0$ отвергается $\rightarrow$ гипотеза о незначимости коэффициента регрессии отвергается (параметр $b$ не случайно отличается от нуля, и сформировался под влиянием систематически действующего фактора)
\end{enumerate}

Критерий опровержения гипотезы:

\begin{equation*}
	|t_{b}| = \frac {b} {m_b} = \frac {b} {\sqrt {\frac{MS_E}{\sum (X-\overline{X})^2}}} > t_{table} \Leftrightarrow H_o \text { discards} \eqno (26)
\end{equation*} 

Величина $m_b$ называется случайной ошибкой коэффициентов регресии. 
Если $t_b > 3 $, то параметры всегда значимы.
\subsubsection{Связь F и t-критериев}
$F$-критерий Снедекора и $t$-критерия Стюдента для коэффициентов регрессии взаимосвязаны. Покажем эту связь:
$$t_b^2 = \frac{b^2}{m_b^2} = \frac {b^2}{\frac{\frac{\sum (Y - \hat{Y})^2} {n - 1 - m}}{\sum (X-\overline{X})^2}}  =  \frac {b^2 \cdot {\sum (X-\overline{X})^2}}{\frac{\sum (Y - \hat{Y})^2} {n - 1 - m}} \stackrel{\ref{SSR}}{=} \frac{SS_R}{\frac{\sum (Y - \hat{Y})^2} {n - 1 - m}} = \frac{MS_R}{MS_E} = F $$

Следовательно:
\label{svyaz_tb_F}
$$t_b = \sqrt {F} \eqno (27)$$

\subsubsection{Гипотеза о коэффициенте коррелляции}

Значимость линейного коэффициента корреляции проверяется на снове величины \textbf{ошибки коэффициента корреляции $m_r$}:
$$ m_r = \sqrt{\frac{1-r_{yx}^2}{n-1-m}} \eqno (28)$$

Фактическое значение $t$-критерия Стьюдента определяется как:
$$ t_r = \frac{r}{\sqrt{1-r_{yx}^2}} \cdot \sqrt{n-1-m} \eqno (29) $$
$$ F = \frac{r_{yx}^2}{(1-r_{yx}^2)} \cdot {(n-1-m)}$$

Для парной регрессии:
$$ t_r = \frac{r}{\sqrt{1-r_{yx}^2}} \cdot \sqrt{n-2} \eqno (30)$$
$$ F = \frac{r_{yx}^2}{(1-r_{yx}^2)} \cdot {(n-2)}$$

Следовательно F и t связаны для коэффициентов корреляции:
$$t_r = \frac{r_{xy}}{m_r}$$
$$ t_r^2 = F , t_b^2 = F \Rightarrow t_r^2 = t_b^2$$
$$ t_r = t_b \eqno (31)$$

Таким образом, проверка гипотез о значимости коэффициентов регрессии и корреляции равносильна проверке гипотезы о существенности линейного уравнения регрессии.
В гипотезе о корреляции: если гипотеза неверна, то зависимость является достоверной и коэффициент корреляции существенно отличен от нуля.

Рассмотренная формула оценки коэффициента корреляции работает при большом числе наблюдений и если $r$ не близко к $\pm 1$. Если же величина коэффициента корреляции близка к $1$, то распределение его оценок отличается от нормального или распределения Стьюдента, так как величина коэффициента корреляции ограничена $[-1,1]$. 

Чтобы обойти это затруднение было предложено для оценки существенности $r$ ввести вспомогательную величину $z$, связанную с коэффициентом корреляции следующим отношением:
$$ z = \frac{1}{2} \cdot ln \frac{1+r}{1-r} \eqno (32)$$ 

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[title = z-grid,  grid = none,scale=1,xmin=-1,xmax=1,ymin = -1,ymax = 1, axis lines=middle, enlargelimits,xlabel = {$x$}, ylabel = {$y$},]
		\addplot[model] {log10((1+x)/(1-x))} ;
		\end{axis}
	\end{tikzpicture}
\end{center}

Величина $z$ изменяется от $ -\infty $ до $+\infty$, что соответствуе пределам нормального распределения. 

Стандартная ошибка величины $z$ вычисляется по формуле:
$$ m_z = \frac{1}{\sqrt{n-3}} \eqno (33)$$

Далее выдвигается нулевая гипотеза $H_0$, которая состоит в том, что корреляция отсутствует, т.е теоретическое значение коэффициента корреляции равно 0:
$$ H_0: r_{xy} = 0, H_1: r_{xy} \neq 0 $$

Критерий опровержения гипотезы:
$$ t_z = \frac{z}{m_z} = z \cdot \sqrt{n-3} \sim t(n-2) \eqno (34) $$
$$ t_z > t_{\alpha} \Leftrightarrow H_0 \text{ discards} $$

\textbf{Вывод}: таким образом, если $H_0$ отвергается, то коэффициент корреляции значимо отличен от нуля.

\subsubsection{Доверительные интервалы для коэффициентов регрессии}
Если коэффициенты регрессии оказываются статистически значимыми, то можно построить \textbf{доверительный интервал} для коэффициентов регрессии:
$$\delta_b = \pm t_{table} \cdot {m_b}$$
$$ b - t_{1 - \frac{\alpha}{2}} \cdot {m_b} \leq b \leq b + t_{1 - \frac{\alpha}{2}} \cdot {m_b} \eqno (35)$$

Также стандартную среднюю ошибку для коэффициента $a$ можно выразить через $m_b$:
$$ m_a = \sqrt{\frac{\sum (Y - \hat{Y})^2} {n - 1 - m} \cdot \frac{\sum X^2}{n\cdot (X-\overline{X})}} = m_b \cdot \sqrt{\frac{\sum X^2}{n}} \eqno (36)$$
\subsubsection{Использование модели парной регрессии для прогнозирования}
В прогнозных расчетах по уравнению регрессии определяется предсказываемое ($y_p$) значение как точечный прогноз $\hat{y_x}$ при $x_p=x_k$, т.е путем подстановки в уравнение регрессии $\hat{y_x} = a+b \cdot x$ соответствующего значения $x$. Однако точечный прогноз явно нереален, поэтому он дополняется расчетом стандартной ошибки $\hat{y_i}$, т.е $m_{\hat{y}}$ и соответственно интервальной оценкой прогнозированного значения $y^*$.

Выражение для \textbf{стандартной ошибки предсказываемого по линии регрессии значения} $\hat{y}$:
$$ m_{\hat{y_x}} = \sqrt{MS_E} \sqrt{\frac{1}{n}+\frac{(x_k-\overline{X})^2}{\sum(X-\overline{X})^2}} \eqno (37) $$

где $\sqrt{MS_E}$ - стандартная ошибка линейной регресии.
Данная формула стандартной ошибки предсказываемого значения $y$ при заданном значении $x_k$ и характеризует ошибку положения линии регрессии.

Величина стандартной ошибки достигает минимума при $x_k  = \overline{X} $.

Для прогнозируемого значения $\hat{y}$ доверительный интервал выглядит следующим бразом:
$$ \hat{y_{x_k}} \pm t_{1-\frac{\alpha}{2}} \cdot m_{\hat{y_x}} $$
$$ \hat{y_{x_k}} - t_{1-\frac{\alpha}{2}} \cdot m_{\hat{y_x}} \leq \hat{y_{x_k}} \leq \hat{y_p} + t_{1-\frac{\alpha}{2}} \cdot m_{\hat{y_x}}  \eqno (38)$$

где:
$$ \hat{y_{x_k}} = a+b\cdot x_k $$
\textbf{Средняя ошибка прогнозируемого индивидуального значения} составит:
$$ m_y = \sqrt{MS_E} \sqrt{1+\frac{1}{n}+\frac{(x_k-\overline{X})^2}{\sum(X-\overline{X})^2}} \eqno (39)$$

\textbf{Доверительный интервал для $y_p$ }- предсказываемого значения регрессии:
$$ \hat{y_p} - t_{\alpha}m_y \leq y_p \leq \hat{y_p} + t_{\alpha}m_y \eqno (40) $$

\subsubsection{Пример использования полученных знаний}

Рассмотрим выборку $\{X,Y\}$, где:
\label{formula2}
$$ X = \{1,2,4,3,5,3,4\} $$ 
$$ Y = \{30,70,150,100,170,100,150\} $$ 
Последовательно проведем анализ согласно изучению материала:

\begin{center}\textbf{1. Найдем оценку параметров модели методом МНК:}\end{center}

Согласно формуле (7) получаем следующую систему уравнений:
$$ \left\{
\begin{matrix}
770 = 7a + 22b \\
2820 = 22a + 80b \\
\end{matrix} \right. $$

Из данной системы уравнений находим значения параметров регрессии $a$ и $b$:
$$ a = -5.78947; b = 36.8421$$

Можно убедиться, что все альтернативные формулы (8) дают те же значения коэффициентов линейной регрессии. 

Построим график прямой $$\hat{Y} = -5.78947 + 36.8421X$$
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[xmin = 0, xmax = 5, grid = major,scale = 1.5,legend pos = north west]
		\legend{ 
	Уравнение регрессии, 
	Начальные данные
	};
	\addplot[model]{-5.78947+36.8421*x};
	\addplot[scatter,only marks] coordinates { (1,30) (2,70) (4,150) (3,100) (5,170) (3,100) (4,150)} ;
		\end{axis}
	\end{tikzpicture}
\end{center}

Линейный коэффициент корреляции по формуле (10):
$$ r = 0.991189$$

\textbf{Вывод}: связь очень высокая и близкая к функциональной.

\begin{center}
\textbf{2. Качество модели} 
\end{center}

По формуле (13) найдем общую сумму квадратов отклонений и объясненную и необъясненную дисперсию:
$$ SS_T = 15000 , SS_R = 14736.8, SS_E = 263.158$$
$$ SS_T = SS_R + SS_E:  \text{ True}$$ 
По формуле (11) и (14) найдем \textit{коэффициент детерминации}:
$$ R^2 = \frac{SS_R}{SS_T} = 0.982456$$ 

\textbf{Вывод}: уравнением регрессии объясняется около $98\%$ дисперсии результативного признака, а на долю других факторов уходит лишь $2\%$ ее дисперсии. Чем больше коэффициент детерминации, тем меньше роль прочих факторов и, следовательно, линейная модель хорошо аппроксимирует данные и ею можно пользоваться для прогноза значений $Y$.

\begin{center}
\textbf{3. Проверка гипотезы о достоверности регрессионной модели} 
\end{center}

Допустим, что $H_0: r^2 = 0$ (как следствие, коэффициент $b=0$).

Выберем уровень значимости: $\alpha = 0.05$

Согласно формулам (17-19), высчитаем дисперсию на одну степень свободы :
$$ MS_R = \frac{SS_R}{1} = 14736.8 $$
$$ MS_E = \frac{SS_E}{n-1-m} = \frac{263.158}{7-2} = 52.6316 $$

Посчитаем статистику $F$-критерия: $$ F_{stat} = \frac{MS_R}{MS_E} = \frac{14736.8}{52.6316} = 280$$

Найдем табличное значение распределения Фишера-Снедекора при заданном уровне значимости:
$$ F_{1-\alpha}(m,n) = F_{0.95}(1,5) = 6.61$$

\textbf{Вывод}: так как $F_{stat}  > F_{0.95} $, то нулевая гипотеза $H_0$ отвергается и делается вывод о том, что регрессия статистически значима и связь существенна.

Также можно проверить, что значение $F$-критерия одинаково и при других альтернативе формулы (23)(можно проверить). Дисперсионная таблица представлена на странице \pageref{first_table_analiz}.

\begin{center}
\textbf{4. Проверка гипотезы о достоверности регрессионной модели} 
\end{center}

Проверим значимость отдельных параметров регрессии, в данном случае значимость параметра $b$.

Введем нулевую гипотезу о том, что параметр регрессии незначим:
\begin{center} $H_0: b=0$. \end{center}

Выберем уровень значимости: $\alpha = 0.05$

Вычислим стандартную ошибку по формуле (24), чтобы построить $t_{stat}$:
$$ m_b = \sqrt { \frac{52.6316}{10.8571}} = 2.20174 $$

Вычислим $t_{stat}$ по формуле (25) :
$$t_b = \frac{b}{m_b} = \frac{36.8421}{2.20174} = 16.7332 $$
$$t_b = \sqrt {F} = \sqrt{280} = 16.7332 $$

Вычислим табличное значение распределения Стьюдента с $(n-2)$ степенями свободы:
$$ t_{table} = t_{1 - \frac{\alpha}{2}} = t_{0.975} = 2.57058$$  

\textbf{Вывод}: так как $t_b > t_{table}$, то нулевая гипотеза $H_0$ отвергается и делается вывод, что коэффициент линейной регрессии $b$ статистически значим

Доверительный интервал для коэффицициента $b$ выглядит следующим образом, согласно формуле (35):
$$36.8421 - 2.57058 \cdot 2.20174 \leq b \leq 36.8421 + 2.57058 \cdot 2.20174  $$
$$31.1824 \leq b \leq 42.5019 $$

\begin{center}
\textbf{5. Использование модели парной регрессии для прогнозирования} 
\end{center}

Вычислим стандартную ошибку предсказываемого по линии регрессии значения $\hat{Y}$ по формуле (37):
$$ m_{\hat{y_x}} = \sqrt{52.6316} \sqrt{\frac{1}{7} + \frac{(x_k-3.14286)^2}{10.8571}} $$

Подставляя различные значения из выборки $X$ мы можем узнать ошибку предсказываемого значения. Минимальная ошибка будет при подстановке $x_k = \overline{X}=3.14286$:
$$ m_{y_{\overline{X}}} = \sqrt{52.6316} \sqrt{\frac{1}{7}} = 2.74204$$

Построим доверительный интервал для $\hat{Y}$ при каком-то произвольном значении $x_k$, например $x_k = 4$. Воспользуемся формулой (38).

Сначала вычислим значение линейной регресии в точке $x_k=4$:
$$ \hat{y_{4}} = -5.78947 + 36.8421\cdot4 = 141.579$$

Затем вычислим стандартную ошибку в точке $x_k=4$:
$$ m_{\hat{y_4}} = \sqrt{52.6316} \sqrt{\frac{1}{7} + \frac{(4-3.14286)^2}{10.8571}} = 3.32871$$

Теперь можно и построить доверительный интервал для уровня значимости $\alpha=0.05$:
$$ \hat{y_{4}} - t_{0.975}\cdot m_{\hat{y_4}} \leq \hat{y_{4}} \leq \hat{y_{4}} + t_{0.975}\cdot m_{\hat{y_4}}$$
$$133.022 \leq \hat{y_{4}} \leq150.136 $$

Значения будут удаляться от линии регрессии по гиперболе, с минимум ошибки в точке $x_k = \bar{X}$. Изобразим это на графике:

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[xmin = 0.8, legend pos = north west, scale = 2, xmax = 5.1,title=График стандартной ошибки и доверительные интервалы, grid = major]
		\legend{ 
	Уравнение регрессии, 
	Начальные данные, 
	Нижняя граница доверительного интервала, 
	Верхняя граница доверительного интервала
	};
	\addplot[model]{-5.78947+36.8421*x};
	\addplot[scatter,only marks] coordinates { (1,30) (2,70) (4,150) (3,100) (5,170) (3,100) (4,150)} ;
	\addplot[orange] coordinates {(0.6, 0.290471) (0.7, 4.48097) (0.8, 8.66721) (0.9, 12.8487) (1,17.0251) (2,58.328) (3,97.642) (4,133.022) (5,165.765) (5.1, 168.976) (5.2, 172.179) (5.3, 175.376) (5.4, 178.567) (5.5, 181.754) (5.6, 184.935) (5.7, 188.113) (5.8, 191.286) (5.9, 194.456) (6, 197.623) } ;
	\addplot[red] coordinates { (0.6, 32.3411) (0.7, 35.519) (0.8, 38.7012) (0.9, 41.8881) (1,45.0802) (2,77.4615) (3,111.832) (4,150.136) (5,191.077)} ;
		\end{axis}
	\end{tikzpicture}
\end{center}


Вычислим среднюю ошибку прогноза по формуле (39) и построим доверительный интвервал. Все действия аналогичны разобранному пункту.

Средняя ошибка прогноза:
$$ m_{\hat{y_x}} = \sqrt{52.6316} \sqrt{1 + \frac{1}{7} + \frac{(x_k-3.14286)^2}{10.8571}} $$

Доверительный интервал для $x_k=4$ и уровня значимости $\alpha=0.05$ по формуле (40):

$$121.061 \leq \hat{y_{x_{k=4}}} \leq162.097 $$

Построим график средней ошибки в зависимости от наблюдений.

\begin{center}
	\begin{tikzpicture}
		\begin{axis}[xmin = 0.8, legend pos = north west, scale = 2, xmax = 5.1,title=График средней ошибки и доверительные интервалы, grid = major]
		\legend{ 
	Уравнение регрессии, 
	Начальные данные, 
	Нижняя граница доверительного интервала, 
	Верхняя граница доверительного интервала
	};
	\addplot[model]{-5.78947+36.8421*x};
	\addplot[scatter,only marks] coordinates { (1,30) (2,70) (4,150) (3,100) (5,170) (3,100) (4,150)} ;
	\addplot[orange] coordinates { (1,7.71691) (246.9351) (3,84.7839) (4,121.061) (5,155.883) } ;
	\addplot[red] coordinates { (1,54.3884) (2,88.8544) (3,124.69) (4,162.097) (5,200.959)} ;
		\end{axis}
	\end{tikzpicture}
\end{center}

\textbf{Вывод}: таким образом, было разобрано, как делать доверительные интвералы для ошибки прогнозирования с помощью стандартной ошибки и средней ошибки прогнозирования.
\newpage
\subsection{Нелинейная регрессия}
Будут рассмотрены два вида \textbf{нелинейной регресиии}:
\begin{enumerate}
	\item \label{1} Нелинейная по независимым переменным
	\item Нелинейная по оцениваемым параметрам 
\end{enumerate}

Для \ref{1} проводится линеаризация:
$$ y = a+ \frac{b}{x} = a+b \cdot \frac{1}{x} = a+b \cdot b(z) \Rightarrow y(z) - \text{linear regression}$$

При построении модели регрессии часто расчитывается показатель эластичности. Только у степенной функции показатель эластичности равен константе. Во всех остальных случаях показатель эластичности зависит от $x$:
$$ \varepsilon = \frac{\mathrm{dy} }{\mathrm{d} x} \cdot{\frac{x}{y}}$$
\end{document}
